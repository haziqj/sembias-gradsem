---
title: 'Bias-reduced estimation of structural equation models'
# subtitle: A statistical perspective
format:
  kaust-revealjs:
    slide-level: 2
    transition: fade
    auto-stretch: false
    width: 1250  # 1050
    height: 760  # 700
    self-contained: false
    chalkboard: true
    toc: false
    toc-depth: 1
    # multiplex: true
    code-block-height: 700px
    # html-table-processing: none
author:
  - name: Haziq Jamil
    orcid: 0000-0003-3298-1010
    affiliations: 
      - 'Research Specialist, BAYESCOMP @ CEMSE-KAUST'
      - '<span style="font-style:normal;">[`https://haziqj.ml/sem-bias/`](https://haziqj.ml/sem-bias/)</span>'
date: 2025-10-16
bibliography: refs.bib
execute:
  echo: false
  freeze: auto
  cache: true
---

## 

```{r}
#| include: false
source("R/sigma2.R")
source("R/score.R")
source("R/adjscore.R")
source("R/results.R")
```


::: {.columns}

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://lavaan.org" target="_blank">
    <img
      src="https://science-academy.ugent.be/sites/acugain/files/styles/medium_1_1/public/teachers/rosseel_yves.jpg?h=04d92ac6&itok=GUYp9IIm"
      alt="Yves Rosseel"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>
    Yves Rosseel<br><em>Universiteit Gent</em> | R/<code>{lavaan}</code>
  </figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <img
    src="https://warwick.ac.uk/fac/sci/statistics/staff/research_students/kemp/img_warwick.jpg"
    style="--size:220px; width:var(--size); height:var(--size);
           object-fit:cover; object-position:top;"
  >
  <figcaption>Ollie Kemp<br><em>University of Warwick</em></figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://www.ikosmidis.com" target="_blank">
    <img
      src="https://www.ikosmidis.com/img/me.jpg"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>Ioannis Kosmidis<br><em>University of Warwick</em></figcaption>
</figure>
:::

:::

::: {.nudge-up}

> **Jamil, H.**, Rosseel, Y., Kemp, O., & Kosmidis, I. (2025). Bias-Reduced Estimation of Structural Equation Models. *Manuscript in Submission*. [`arXiv:2509.25419`](https://doi.org/10.48550/arXiv.2509.25419).

- Source: <https://github.com/haziqj/sembias-gradsem>
- R Package: <https://github.com/haziqj/brlavaan>

:::

[{{< placeholder 220 220 format=svg >}}]{.absolute bottom=10 right=0}

[[**poll**]{.bg-grn}]{.absolute bottom=220 right=235}

## Context

::: {.callout-note icon=false title="SEM in a nutshell"}
Analyse multivariate data $\mathbf y=(y_1,\dots,y_p)^\top$ to measure and relate hidden variables $\boldsymbol\eta=(\eta_1,\dots,\eta_q)^\top$, $q \ll p$, and uncover complex patterns.
:::

::: {.nudge-up-small}
In the social sciences, latent variables are used to represent **constructs**---the *theoretical, unobserved* concepts of interest.
:::

::: {.nudge-up}
:::: {.columns}

::: {.column width="25%"}
![*(Psychology)*<br>Personality traits](https://unsplash.com/photos/5bYxXawHOQg/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzMTM4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Healthcare)*<br>Quality of life](https://unsplash.com/photos/hIgeoQjS_iE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjI2Nzg4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Political science)*<br>Social trust](https://unsplash.com/photos/zjeZXMU1SKE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzIxfA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Education)*<br>Competencies](https://unsplash.com/photos/oXV3bzR7jxI/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzM1fA&force=true&w=640){height=210px}
:::

::::
:::

::: {.aside}
Photo credits: Unsplash
[\@dtravisphd](https://unsplash.com/photos/brown-fountain-pen-on-notebook-5bYxXawHOQg),
[\@impulsq](https://unsplash.com/photos/doctor-holding-red-stethoscope-hIgeoQjS_iE),
[\@ev](https://unsplash.com/photos/a-group-of-police-standing-next-to-each-other-zjeZXMU1SKE),
[\@benmullins](https://unsplash.com/photos/person-using-pencil-oXV3bzR7jxI).
:::

## Motivation

*"Using SEMs in empirical research is often challenged by small sample sizes."*

- Why? Data collection is expensive, time-consuming, or difficult.
- Rare populations:
    - **@quezada2016explanatory:** Identifying factors of adjustment in pediatric burn patients to facilitate appropriate mental health interventions postinjury ($n=51$).
    - **@figueroa2021structural:** Studying functional connectivity network on individuals with rare genetic disorders ($n=22$).
    - **@fabbricatore2023componentbased**: Assessment of psycho-social aspects and performance of elite swimmers ($n=161$).
    - **@manuela2013pacific**: Validating self-report measures of identity on a unique cultural group ($n=143$).
- SEM is desirable, but small $n \Rightarrow$ poor finite-sample performance (esp. bias). 

## Outline

1. Brief overview of SEMs
    - Model equations
    - ML estimation and inference
    - Examples of SEMs
      - Two-factor SEM
      - Latent growth models
2. Bias reducing methods
    - A review
    - Resampling-based methods
    - Alternative approaches
    - Reduced-Bias $M$-estimation (RBM)
3. Simulation studies and results

# Structural equation models {.transition-slide}

## Motivating example

### Glycemic control and kidney health

> Does poorer glycemic control lead to greater severity of kidney disease?


::: {.columns}

::: {.column width=48%}
For $i=1,\dots,n$ patients, observe $p=6$ variables:

```{r}
#| html-table-processing: none
library(gt)

tbl <- data.frame(
  Variable = c("$x_1$", "$x_2$", "$x_3$", "$y_1$", "$y_2$", "$y_3$"),
  Indicator = c("HbA1c", "FPG", "Insulin", "PCr", "ACR", "BUN"),
  Description = c(
    "3-month avg. blood glucose",
    "Fasting plasma glucose",
    "Fasting insulin level",
    "Plasma creatinine",
    "Albumin–creatinine ratio",
    "Blood urea nitrogen"
  ),
  Unit = c("%", "mmol/L", "µU/mL", "µmol/L", "mg/g", "mmol/L"),
  stringsAsFactors = FALSE
)

tbl |>
  gt() |>
  # tab_header(title = md("**Observed Variables**")) |>
  cols_label(
    Variable   = "Var.",
    Indicator  = "Indicator",
    Description = "Description",
    Unit       = "Unit"
  ) |>
  fmt_markdown(columns = "Variable") |>
  tab_options(
    table.font.size = px(25),
    table.width = pct(100)
  ) |>  
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) |>
  opt_table_font(font = list(google_font("Raleway"), default_fonts()))
```

:::

::: {.column width=2%}
:::

::: {.column width=50%}
::: {.nudge-down}
"*casewise*" thinking leads to
$$
\small
\begin{align*}
y_{1} &= \beta_0^{(1)} + \beta_1^{(1)} x_1 + \beta_2^{(1)} x_2 + \beta_3^{(1)} x_3 + \epsilon^{(j)} \\
y_{2} &= \beta_0^{(2)} + \beta_1^{(2)} x_1 + \beta_2^{(2)} x_2 + \beta_3^{(2)} x_3 + \epsilon^{(j)} \\
y_{3} &= \beta_0^{(3)} + \beta_1^{(3)} x_1 + \beta_2^{(3)} x_2 + \beta_3^{(3)} x_3 + \epsilon^{(j)} \\
\end{align*}
$$

- Does not give a clear and direct answer.

- Moreover, $x$'s are assumed to be measured without error!

:::
:::

:::




::: aside
Example adapted from @song2012basic.
:::

## Covariance-based approach

::: {.columns}

::: {.column width=45%}
::: {.nudge-up-small}
Sample correlation matrix looks like this^[Simulated data, from a two-factor SEM.]:

```{r}
#| fig-align: center
#| out-width: 100%
#| fig-height: 4.5
#| fig-width: 4.5
library(brlavaan)
dat <- gen_data_twofac(n = 1000)
z <- cor(dat)
colnames(z) <- rownames(z) <- c("x[1]","x[2]","x[3]","y[1]","y[2]","y[3]")

library(ggcorrplot)
ggcorrplot(
  z[, 6:1],
  lab = TRUE, lab_col = "black", lab_size = 4,
  colors = c("#00A6AA", "white", "#F18F00"),
  type = "full",    
  show.diag = TRUE
) +
  scale_x_discrete(position = "top", labels = scales::label_parse()) +
  scale_y_discrete(labels = scales::label_parse()) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    axis.ticks = element_blank(),
    legend.position = "none"
  )
```
:::
:::

::: {.column width=55%}
::: {.nudge-down-medium}
- The data suggests clustering of variables
   - $x$'s measure *glycemic control*
   - $y$'s measure *kidney health*

- There is an element of dimension-reduction, much needed for analysing multivariate data.

- Easier to hypothesize relationships, e.g.
  $$
  \texttt{KdnHlt} = \alpha + \beta \,  \texttt{GlyCon} + \texttt{error}
  $$


- SEM is about modelling the <u>covariance structure</u> of the data, 
$$
\boldsymbol\Sigma = \boldsymbol\Sigma(\vartheta).
$$

:::
:::

:::







## Structural model

## Example 1: Political democracy example

## ML estimation

## Properties of MLE

Let $\bar{\vartheta}$ be the true parameter value.
Subject to standard regularity conditions [@cox1979theoretical], as $n\to\infty$,

$$
\sqrt n (\hat\vartheta - \bar\vartheta) \xrightarrow{\;\;\text D\;\;} 
\begin{cases}
\text N_m\left(\mathbf 0, \big[ U(\bar\vartheta)V(\bar\vartheta)^{-1} U(\bar\vartheta) \big]^{-1} \right) &\text{model misspecified} \\
\text{N}_m\big(\mathbf 0, I(\bar\vartheta)^{-1} \big) &\text{otherwise}
\end{cases}
$$
where

- $I(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *Fisher information*;
- $U(\vartheta) = -\mathbb{E}\left[ \nabla\nabla^\top \ell_1(\vartheta) \right]$ is the *sensitivity matrix*; and
- $V(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *variability matrix*.

::: {.nudge-up}
Calculation of SEs are based off estimates of these matrices.
The "sandwich" matrix gives robust SEs [@satorra1994corrections;@savalei2014understanding].
:::


## Example 2: Latent growth curve model

## REML

# Bias reduction methods {.transition-slide}

## What is bias?

::: {.callout-tip icon=false}
#### Bias of an estimator
$$
\mathcal B_{\bar\vartheta}(\hat\vartheta) = \mathbb E\left[\hat\vartheta - \bar\vartheta\right] 
$${#eq-bias}
:::


Consider the stochastic Taylor expansion of $s(\hat\vartheta)=\nabla\ell(\vartheta)=0$ around $\bar\vartheta$.
For many common estimators including MLE, the bias function is:
$$
\mathcal B_{\bar\vartheta} = \frac{b_1(\bar\vartheta)}{n} +  \frac{b_2(\bar\vartheta)}{n^2} + \frac{b_3(\bar\vartheta)}{n^3} +O(n^{-4}).
$$

::: {.nudge-up}
Bias arises because the roots of the score equations are **not exactly centred at $\bar\vartheta$**, due to:

::: {.nudge-up-small}
a. The curvature of the score $s(\vartheta)$ creating asymmetry; and
b. The randomness of the score itself.
:::
:::


## Illustration

### Biased MLE estimator for $\sigma^2$ 

Consider $X_1,\dots,X_n \sim \text N(0, \sigma^2)$. The MLE for $\sigma^2$ is $\hat\sigma^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 5)
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 25)
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 1000)
```

:::

## Illustration (cont.)

### Score functions are random too

The score is $s(\sigma^2)=\ell'(\sigma^2) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_score(n = 15) + coord_cartesian(ylim = c(-8, 28))
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 50, showbias = FALSE) + coord_cartesian(ylim = c(-8, 300))
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 1000, showbias = FALSE)
```

:::

## If you're interested... 

### ...and love differentiation ️❤️🤓

For a comprehensive treatment of bias-reduction methods,

- Start here: @cox1968general
- Follow up with: @firth1993bias; @kosmidis2009bias; @kosmidis2014bias


::: {.fragment}
::: {.nudge-up}
By the way, the $O(n^{-1})$ bias term $b_1(\bar\vartheta)/n = -I(\bar\vartheta)^{-1} C(\bar\vartheta)$, where
$$
\begin{gathered}
C_a(\vartheta) = \frac{1}{2} \operatorname{tr} \left[
I(\vartheta)^{-1}\big( G_a(\vartheta) + H_a(\vartheta) \big)
\right]\\ 
G_a(\vartheta)=\mathbb E[s(\vartheta)s(\vartheta)^\top s_a(\vartheta)] 
\quad\quad 
H_a(\vartheta)=-\mathbb E[I(\vartheta)s_a(\vartheta)] \\
a=1,\dots,m
\end{gathered}
$$
where $s(\vartheta) = \nabla\ell(\vartheta)$ is the score function.
:::
:::

## A review

::: {.nudge-up-large}
![](figures/bias.png){fig-align=center width=67%}
:::

::: {.nudge-up-small}
::: {.nudge-up-large}
```{r}
#| html-table-processing: none
library(gt)
library(dplyr)

df <- tribble(
  ~Method, ~Model, ~BG_theta0, ~Type, ~E, ~d, ~theta_hat,
  "Asymptotic bias correction", "full", "analytical", "explicit", "✓","✓","✓",
  "Adjusted score functions", "full", "analytical", "implicit", "✓","✓","✗",
  "Bootstrap", "partial", "simulation", "explicit", "✗","✗","✓",
  "Jackknife", "partial", "simulation", "explicit", "✗","✗","✓",
  "Indirect inference", "full", "simulation", "implicit", "✗","✗","✓",
  "Explicit RBM", "partial", "analytical", "explicit", "✗","✓","✓",
  "Implicit RBM", "partial", "analytical", "implicit", "✗","✓","✗"
)

gt(df, rownames_to_stub = TRUE) |>
  text_transform(
    locations = cells_body(columns = c(E, d, theta_hat)),
    fn = \(x) {
      x |>
        str_replace("✓", '<span style="color:#004C59">✓</span>') |>
        str_replace("✗", '<span style="color:#b10f2e">✗</span>')
      # gt::html(out) 
  }) |>
  tab_spanner(md("**Requirements**"), columns = c(E, d, theta_hat)) |>
  cols_label(
    BG_theta0 = md("$\\mathcal{B}(\\bar\\vartheta)$"),
    E = md("$\\mathbb{E}(\\cdot)$"),
    d = md("$\\hspace{2pt} \\partial \\cdot \\hspace{2pt}$"),
    theta_hat = md("$\\hspace{4pt} \\hat\\vartheta \\hspace{4pt}$")
  ) |>
  cols_align(
    align = "center",
    columns = c(E, d, theta_hat)
  ) |>
  tab_style(
    style = "font-weight: bold",
    locations = cells_column_labels()
  ) |>
  tab_options(
    table.font.size = px(25),
    table.width = pct(95)
    # data_row.padding = px(4)
  ) |>
  opt_table_font(
    font = list(
      google_font("Raleway"),  # loads the font for the table
      default_fonts()          # sensible fallbacks
    )
  )
```
:::
:::

::: aside
::: {.footnotesize-text}
1--@efron1975defining, @cordeiro1991bias; 2--@firth1993bias, @kosmidis2009bias; 3--@efron1994introduction, @hall1988bootstrap; 4--@quenouille1956notes, @efron1982jackknife; 5--@gourieroux1993indirect, @mackinnon1998approximate
:::
:::

## Firth's adjusted scores methods

::: {.nudge-up}
Instead of solving [$s(\vartheta)=0$]{.text-org}, solve [$s(\vartheta) + \overbrace{A(\vartheta)}^{\mathcal B(\vartheta) I(\vartheta)}=0$]{.text-tur}.
:::

<!-- ```{r} -->
<!-- #| out-width: 100% -->
<!-- #| fig-align: center -->
<!-- #| fig-height: 4 -->
<!-- #| fig-width: 8 -->
<!-- plot_adjusted_score() -->
<!-- ``` -->

::: {.nudge-up}
![](figures/adjusted_score.gif){fig-align=center width=100%}
:::

## Implicit RBM estimator

Computing $\mathcal B(\vartheta)$ and $I(\vartheta)$ can be difficult. 
Consider

$$
s(\vartheta) + A(\vartheta) = 0 \ \Leftrightarrow \ \underset{\vartheta}{\text{arg max}} \left \{ \ell(\vartheta) + P(\vartheta) \right\}
$${#eq-irbm}

where $P(\vartheta)$ is a penalty term constructed such that $A(\vartheta) = \nabla P(\vartheta)$. 
@kosmidis2024empirical show that
$$
P(\vartheta) = -\frac{1}{2} \operatorname{tr} \Big\{ j(\vartheta)^{-1} e(\vartheta) \Big\}
$$ 

where

::: {.nudge-up-small}
- $j(\vartheta) = \sum_{i=1}^n \nabla\nabla^\top \ell_i(\vartheta)$ is the observed information;
- $e(\vartheta) = \sum_{i=1}^n \nabla\ell_i(\vartheta)\nabla^\top\ell_i(\vartheta)\nabla\ell_i(\vartheta)$ is the outer-product of scores.

::: {.nudge-up-small}
The solution $\tilde\vartheta$ to (-@eq-irbm) is called the [*implicit* RBM estimator (iRBM)]{}.
:::
:::

## Explicit RBM estimator

Intuitively, by thinking in terms of a Newton-style update, an *explicit* estimator is obtained via
$$
\vartheta^*= \hat\vartheta + j(\hat\vartheta)^{-1} A(\hat\vartheta).
$$

This moves $\hat\theta$ in the direction $A(\hat\vartheta)$ away from the bias, with step length governed by the curvature $j(\hat\vartheta)^{-1}$.

::: {.columns}

::: {.column width=50%}
{{< placeholder 600 300 format=svg >}}
:::

::: {.column width=50%}
- Operationally, eRBM is simpler and quicker to compute than iRBM---no re-optimisation needed if $\hat\theta$ is available.

- However, unlike iRBM, no guarantees that bias correction stays inside the parameter space.
:::

:::

<!-- ## Resampling-based methods -->

# Simulation studies {.transition-slide}

## Simulation design

<!-- - Data generating models: -->
<!--    1. Two-factor SEM ($m=13$ estimable params). -->
<!--    2. Latent growth curve model ($m=6$ estimable params). -->
   
- Sample size: $n\in \{15,20,50,100,1000\}$

- Item reliability: Low or High ($\operatorname{Rel} = p^{-1}\sum_{j=1}^p \boldsymbol\Sigma_{jj}^* / \boldsymbol\Sigma_{jj}$)

- Distributional assumption: Normal or Non-normal

```{r}
#| fig-height: 3.6
#| fig-width: 9
#| fig-align: center
#| out-width: 100%
n <- 1000
rho <- 0.3
Sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
dat <- list()
dat$Normal <-
  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(0, 2))[[1]]
## dat$Kurtosis <-  # kurtosis
##   covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(6, 2))[[1]]
dat$`Non-normal` <-  # non-normal
  covsim::rIG(n, Sigma, skewness = rep(-2, 2), excesskurtosis = rep(6, 2))[[1]]

# get max and min for x and y axes
xyminmax <- 
  map(dat, as.data.frame) |>
  bind_rows() |>
  summarise(
    x_min = min(V1),
    x_max = max(V1),
    y_min = min(V2),
    y_max = max(V2)
  ) |> 
  unlist() 

allplots <- imap(dat, \(x, idx) {
  # mycols <- rev(RColorBrewer::brewer.pal(3, "Set1"))
  mycols <- c("#00A6AA",  "#F18F00", "black")
  names(mycols) <- names(dat)
  
  p <-
    as.data.frame(x) |>
    ggplot(aes(x = V1, y = V2)) +
    geom_point(alpha = 0.5, size = 1, col = mycols[idx]) +
    geom_density_2d(col = "black") +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text = element_blank(),
      axis.ticks = element_blank()
    ) +
    scale_x_continuous(limits = c(xyminmax["x_min"], xyminmax["x_max"])) +
    scale_y_continuous(limits = c(xyminmax["y_min"], xyminmax["y_max"])) +
    labs(title = glue::glue("{idx}"), x = NULL, y = NULL)
  ggExtra:::ggMarginal(p, fill = mycols[idx], alpha = 0.5, size = 10, 
                       trim = idx != "Non-normal") 
})
cowplot::plot_grid(plotlist = allplots, nrow = 1)
```


## Results: Two-factor SEM 

::: {.nudge-up-small}

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.5
#| fig-width: 9
p1
```

:::


## Results: Two-factor SEM (cont.)

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.4
#| fig-width: 9
p2
```

## Results: Latent GCM

::: {.nudge-up}

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.4
#| fig-width: 9
p3
```

:::

# Conclusion {.transition-slide}

## Summary & future work

RBM applied to small sample estimation of SEM show key advantages:

- Improved estimator performances (mean & median bias, RMSE, coverage).

- Computationally efficient (c.f. resampling methods).

- Robust to model misspecification.

Future work include

1. Computational improvements for iRBM.

2. Plugin penalties to limit exploration of ill-conditioned regions.

3. Extension to other SEM families, e.g.
   - Mediation models, latent interactions, etc.
   - Alternative to ML estimation e.g. WLS, DWLS, etc.

## Software 

```{r}
#| include: false
library(brlavaan)
dat <- brlavaan::gen_data_twofac(n = 50)
colnames(dat) <- paste0("y", 1:6)
options(width = 180)
```


```{r}
#| echo: true
#| cache: true
# pak::pak("haziqj/brlavaan")
library(brlavaan)

mod <- "
  eta1 =~ y1 + y2 + y3
  eta2 =~ y4 + y5 + y6
"
fit <- brsem(model = mod, data = dat, estimator.args = list(rbm = "implicit"))
summary(fit)
```

# شكراً جزيلاً {.thanks-slide  background-image="_extensions/haziqj/kaust/KAUST-Thank-you.jpg" style="padding-top:0.5em;padding-bottom:0em"}

[`https://haziqj.ml/sembias-gradsem`](https://haziqj.ml/sembias-gradsem)

## References