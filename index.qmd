---
title: 'Bias-reduced estimation of structural equation models'
# subtitle: A statistical perspective
format:
  kaust-revealjs:
    slide-level: 2
    transition: fade
    auto-stretch: false
    width: 1250  # 1050
    height: 760  # 700
    self-contained: false
    chalkboard: true
    toc: false
    toc-depth: 1
    # multiplex: true
    code-block-height: 700px
    # html-table-processing: none
author:
  - name: Haziq Jamil
    orcid: 0000-0003-3298-1010
    affiliations: 
      - 'Research Specialist, BAYESCOMP @ CEMSE-KAUST'
      - '<span style="font-style:normal;">[`https://haziqj.ml/sem-bias/`](https://haziqj.ml/sem-bias/)</span>'
date: 2025-10-16
bibliography: refs.bib
execute:
  echo: false
  freeze: auto
  cache: true
---

## 

```{r}
#| include: false
source("R/sigma2.R")
source("R/score.R")
source("R/adjscore.R")
source("R/results.R")
```


::: {.columns}

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://lavaan.org" target="_blank">
    <img
      src="https://science-academy.ugent.be/sites/acugain/files/styles/medium_1_1/public/teachers/rosseel_yves.jpg?h=04d92ac6&itok=GUYp9IIm"
      alt="Yves Rosseel"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>
    Yves Rosseel<br><em>Universiteit Gent</em> | R/<code>{lavaan}</code>
  </figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <img
    src="https://warwick.ac.uk/fac/sci/statistics/staff/research_students/kemp/img_warwick.jpg"
    style="--size:220px; width:var(--size); height:var(--size);
           object-fit:cover; object-position:top;"
  >
  <figcaption>Ollie Kemp<br><em>University of Warwick</em></figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://www.ikosmidis.com" target="_blank">
    <img
      src="https://www.ikosmidis.com/img/me.jpg"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>Ioannis Kosmidis<br><em>University of Warwick</em></figcaption>
</figure>
:::

:::

::: {.nudge-up}

> **Jamil, H.**, Rosseel, Y., Kemp, O., & Kosmidis, I. (2025). Bias-Reduced Estimation of Structural Equation Models. *Manuscript in Submission*. [`arXiv:2509.25419`](https://doi.org/10.48550/arXiv.2509.25419).

- Source: <https://github.com/haziqj/sembias-gradsem>
- R Package: <https://github.com/haziqj/brlavaan>

:::

[{{< placeholder 220 220 format=svg >}}]{.absolute bottom=10 right=0}

[[**poll**]{.bg-grn}]{.absolute bottom=220 right=235}

## Context

::: {.callout-note icon=false title="SEM in a nutshell"}
Analyse multivariate data $\mathbf y=(y_1,\dots,y_p)^\top$ to measure and relate hidden variables $\boldsymbol\eta=(\eta_1,\dots,\eta_q)^\top$, $q \ll p$, and uncover complex patterns.
:::

::: {.nudge-up-small}
In the social sciences, latent variables are used to represent **constructs**---the *theoretical, unobserved* concepts of interest.
:::

::: {.nudge-up}
:::: {.columns}

::: {.column width="25%"}
![*(Psychology)*<br>Personality traits](https://unsplash.com/photos/5bYxXawHOQg/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzMTM4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Healthcare)*<br>Quality of life](https://unsplash.com/photos/hIgeoQjS_iE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjI2Nzg4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Political science)*<br>Social trust](https://unsplash.com/photos/zjeZXMU1SKE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzIxfA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Education)*<br>Competencies](https://unsplash.com/photos/oXV3bzR7jxI/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzM1fA&force=true&w=640){height=210px}
:::

::::
:::

::: {.aside}
Photo credits: Unsplash
[\@dtravisphd](https://unsplash.com/photos/brown-fountain-pen-on-notebook-5bYxXawHOQg),
[\@impulsq](https://unsplash.com/photos/doctor-holding-red-stethoscope-hIgeoQjS_iE),
[\@ev](https://unsplash.com/photos/a-group-of-police-standing-next-to-each-other-zjeZXMU1SKE),
[\@benmullins](https://unsplash.com/photos/person-using-pencil-oXV3bzR7jxI).
:::

## Motivation

*"Using SEMs in empirical research is often challenged by small sample sizes."*

- Why? Data collection is expensive, time-consuming, or difficult.
- Rare populations:
    - **@quezada2016explanatory:** Identifying factors of adjustment in pediatric burn patients to facilitate appropriate mental health interventions postinjury ($n=51$).
    - **@figueroa2021structural:** Studying functional connectivity network on individuals with rare genetic disorders ($n=22$).
    - **@fabbricatore2023componentbased**: Assessment of psycho-social aspects and performance of elite swimmers ($n=161$).
    - **@manuela2013pacific**: Validating self-report measures of identity on a unique cultural group ($n=143$).
- SEM is desirable, but small $n \Rightarrow$ poor finite-sample performance (esp. bias). 

## Outline

1. **Brief overview of SEMs**
    - Motivating example
    - ML estimation and inference
    - Examples of SEMs
      - Two-factor SEM
      - Latent growth models
2. **Bias reducing methods**
    - What is bias?
    - A review of bias reduction methods
    - Reduced-Bias $M$-estimation (RBM)
      - Implicit correction
      - Explicit correction
3. **Simulation studies and results**

# Structural equation models {.transition-slide}

## Motivating example

### Glycemic control and kidney health

> Does poorer glycemic control lead to greater severity of kidney disease?


::: {.columns}

::: {.column width=48%}
For $i=1,\dots,n$ patients, observe $p=6$ variables:

```{r}
#| html-table-processing: none
library(gt)

tbl <- data.frame(
  Variable = c("$x_1$", "$x_2$", "$x_3$", "$y_1$", "$y_2$", "$y_3$"),
  Indicator = c("HbA1c", "FPG", "Insulin", "PCr", "ACR", "BUN"),
  Description = c(
    "3-month avg. blood glucose",
    "Fasting plasma glucose",
    "Fasting insulin level",
    "Plasma creatinine",
    "Albumin–creatinine ratio",
    "Blood urea nitrogen"
  ),
  Unit = c("%", "mmol/L", "µU/mL", "µmol/L", "mg/g", "mmol/L"),
  stringsAsFactors = FALSE
)

tbl |>
  gt() |>
  # tab_header(title = md("**Observed Variables**")) |>
  cols_label(
    Variable   = "Var.",
    Indicator  = "Indicator",
    Description = "Description",
    Unit       = "Unit"
  ) |>
  fmt_markdown(columns = "Variable") |>
  tab_options(
    table.font.size = px(25),
    table.width = pct(100)
  ) |>  
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) |>
  opt_table_font(font = list(google_font("Raleway"), default_fonts()))
```

:::

::: {.column width=2%}
:::

::: {.column width=50%}
::: {.nudge-down}
"*casewise*" thinking leads to
$$
\small
\begin{align*}
y_{1} &= \beta_0^{(1)} + \beta_1^{(1)} x_1 + \beta_2^{(1)} x_2 + \beta_3^{(1)} x_3 + \epsilon^{(j)} \\
y_{2} &= \beta_0^{(2)} + \beta_1^{(2)} x_1 + \beta_2^{(2)} x_2 + \beta_3^{(2)} x_3 + \epsilon^{(j)} \\
y_{3} &= \beta_0^{(3)} + \beta_1^{(3)} x_1 + \beta_2^{(3)} x_2 + \beta_3^{(3)} x_3 + \epsilon^{(j)} \\
\end{align*}
$$

- Does not give a clear and direct answer.

- Moreover, $x$'s are assumed to be measured without error.

:::
:::

:::




::: aside
Example adapted from @song2012basic.
:::

## Covariance-based approach

::: {.columns}

::: {.column width=45%}
::: {.nudge-up-small}
Sample correlation matrix looks like this^[Simulated data, from a two-factor SEM ($n=1000$).]:

```{r}
#| fig-align: center
#| out-width: 100%
#| fig-height: 4.5
#| fig-width: 4.5
library(brlavaan)
dat <- gen_data_twofac(n = 1000)
z <- cor(dat)
colnames(z) <- rownames(z) <- c("x[1]","x[2]","x[3]","y[1]","y[2]","y[3]")

library(ggcorrplot)
ggcorrplot(
  z[, 6:1],
  lab = TRUE, lab_col = "black", lab_size = 4,
  colors = c("#00A6AA", "white", "#F18F00"),
  type = "full",    
  show.diag = TRUE
) +
  scale_x_discrete(position = "top", labels = scales::label_parse()) +
  scale_y_discrete(labels = scales::label_parse()) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    axis.ticks = element_blank(),
    legend.position = "none"
  )
```
:::
:::

::: {.column width=55%}
::: {.nudge-down-medium}
- The data suggests clustering of variables
   - $x$'s measure *glycemic control*
   - $y$'s measure *kidney health*

- There is an element of dimension-reduction, much needed for analysing (correlated) multivariate data.

- Easier to hypothesize relationships, e.g.
  $$
  \texttt{KdnHlt} = \alpha + \beta \,  \texttt{GlyCon} + \texttt{error}
  $$


- SEM is about modelling the <u>covariance structure</u> of the data, 
$$
\boldsymbol\Sigma = \boldsymbol\Sigma(\vartheta).
$$
:::
:::

:::

## SEM equations

::: {.columns}

::: {.column width=50%}
::: {.nudge-up}
$$
\small
\begin{gathered}
\begin{pmatrix}
y_1 \\
y_2 \\
y_3 \\
y_4 \\
y_5 \\
y_6
\end{pmatrix}
= 
\begin{pmatrix}
\lambda_{11} & 0 \\
\lambda_{21} & 0 \\
\lambda_{31} & 0 \\
0 & \lambda_{42} \\
0 & \lambda_{52} \\
0 & \lambda_{62} \\
\end{pmatrix}
\begin{pmatrix}
\eta_1 \\
\eta_2
\end{pmatrix}
+ 
\begin{pmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\epsilon_5 \\
\epsilon_6
\end{pmatrix} \\[1em]
\begin{pmatrix}
\eta_1 \\
\eta_2
\end{pmatrix}
=
\begin{pmatrix}
0 & 0 \\
\beta & 0 \\
\end{pmatrix}
\begin{pmatrix}
\eta_1 \\
\eta_2
\end{pmatrix}
+
\begin{pmatrix}
\zeta_1 \\
\zeta_2
\end{pmatrix}
\end{gathered}
$$

Or, more compactly as

::: {.nudge-up}
$$
\begin{gathered}
\mathbf y = {\color{Gray}\boldsymbol\nu +\,}  \boldsymbol\Lambda \boldsymbol\eta + \boldsymbol\epsilon \\
\boldsymbol\eta = {\color{Gray}\boldsymbol\alpha +\,} \mathbf B \boldsymbol\eta + \boldsymbol\zeta
\end{gathered}
$$

with assumptions $\boldsymbol\epsilon\sim\operatorname{N}_p(\mathbf 0, \boldsymbol\Theta)$, $\boldsymbol\zeta\sim\operatorname{N}_q(\mathbf 0, \boldsymbol\Psi)$, and $\operatorname{Cov}(\boldsymbol\epsilon, \boldsymbol\zeta)=\mathbf 0$.

:::



:::
:::

::: {.column width=50%}
```{r, engine = "tikz"}
#| fig-align: center
#| out-width: 100%
\begin{tikzpicture}[%
  scale=0.88,
  >=stealth,
  auto,
  node distance=1.6cm,
  every node/.style={font=\small},
  latent/.style={font=\normalsize,circle, draw, minimum size=9mm, inner sep=0mm},
  obs/.style={font=\normalsize,rectangle, draw, minimum size=6mm, inner sep=0mm},
  error/.style={font=\normalsize,circle, draw, minimum size=5mm, inner sep=0mm}
]

% --------------------------------------------------------
% 1) LATENT VARIABLES
% --------------------------------------------------------
\node[latent] (eta1) at (-1.5, -2) {$\eta_{1}$};
\node[latent] (eta2) at ( 3.5, -2) {$\eta_{2}$};

% --------------------------------------------------------
% 2) OBSERVED VARIABLES + ERROR TERMS
%    Place y1..y3 (for eta1) on the left, y4..y6 (for eta2) on the right.
% --------------------------------------------------------
\node[obs] (y1) at (-2.8, 0) {$y_{1}$};
\node[obs] (y2) at (-1.5, 0) {$y_{2}$};
\node[obs] (y3) at ( -0.2, 0) {$y_{3}$};

\node[obs] (y4) at ( 2.2, 0) {$y_{4}$};
\node[obs] (y5) at ( 3.5, 0) {$y_{5}$};
\node[obs] (y6) at ( 4.8, 0) {$y_{6}$};

% Error terms above each observed variable
\node[error] (e1) at (-2.8, 1) {$\epsilon_{1}$};
\node[error] (e2) at (-1.5, 1) {$\epsilon_{2}$};
\node[error] (e3) at ( -0.2, 1) {$\epsilon_{3}$};
\node[error] (e4) at ( 2.2, 1) {$\epsilon_{4}$};
\node[error] (e5) at ( 3.5, 1) {$\epsilon_{5}$};
\node[error] (e6) at ( 4.8, 1) {$\epsilon_{6}$};

% --------------------------------------------------------
% 3) PATHS FROM ERROR TERMS TO OBSERVED VARIABLES
% --------------------------------------------------------
\draw[->] (e1) -- (y1);
\draw[->] (e2) -- (y2);
\draw[->] (e3) -- (y3);
\draw[->] (e4) -- (y4);
\draw[->] (e5) -- (y5);
\draw[->] (e6) -- (y6);

% --------------------------------------------------------
% 4) FACTOR LOADINGS
%    \eta_{i1} -> y1, y2, y3
%    \eta_{i2} -> y4, y5, y6
% --------------------------------------------------------
\draw[->] (eta1) -- node[pos=0.5, right] {$1$} (y1);
\draw[->] (eta1) -- node[pos=0.5, right] {$\lambda_{21}$} (y2);
\draw[->] (eta1) -- node[pos=0.5, right] {$\lambda_{31}$} (y3);

\draw[->] (eta2) -- node[pos=0.5, right] {$1$} (y4);
\draw[->] (eta2) -- node[pos=0.5, right] {$\lambda_{52}$} (y5);
\draw[->] (eta2) -- node[pos=0.5, right] {$\lambda_{62}$} (y6);

% --------------------------------------------------------
% 5) REGRESSION BETWEEN LATENT VARIABLES
%    \eta_1 -> \eta_2, labeled beta
% --------------------------------------------------------
\draw[->] (eta1) -- node[midway, above] {$\beta$} (eta2);

% --------------------------------------------------------
% 6) VARIANCES OF LATENT VARIABLES
%    Double-headed arrows for psi_{11} and psi_{22}
% --------------------------------------------------------
\draw[<->] (eta1) to[out=200, in=230, looseness=4] 
  node[left] {$\psi_{11}$} (eta1);

\draw[<->] (eta2) to[out=-50, in=-20, looseness=4] 
  node[right] {$\psi_{22}$} (eta2);

\foreach \i in {1,...,6}{
  \draw[<->] (e\i)
    to[out=110, in=140, looseness=5]
    node[above] {$\theta_{\i\i}$}
    (e\i);
}

\end{tikzpicture}
```

- SEM parameters include the free entries of $\boldsymbol\nu$, $\boldsymbol\Lambda$, $\boldsymbol\Theta$, $\boldsymbol\alpha$, $\mathbf B$, and $\boldsymbol\Psi$. 

- Dump all in $\vartheta \in\mathbb R^m$, where $m < p(p+1)/2 {\color{Gray} \,+\, p}$.

- Sometimes, not interested in mean structure, so $\boldsymbol\nu$ and $\boldsymbol\alpha$ are dropped.

:::

:::

## ML estimation

- It can be shown that the normal SEM reduces to $\mathbf y\sim \text{N}_p\big(\boldsymbol\mu(\vartheta), \boldsymbol\Sigma(\vartheta)\big)$, where
$$
\begin{align}
\boldsymbol\mu(\vartheta) &= \boldsymbol\nu + \boldsymbol\Lambda (\mathbf I - \mathbf B)^{-1} \boldsymbol\alpha  \\
\boldsymbol\Sigma(\vartheta) &= \boldsymbol\Lambda (\mathbf I - \mathbf B)^{-1} \boldsymbol\Psi (\mathbf I - \mathbf B)^{-\top} \boldsymbol\Lambda^\top + \boldsymbol\Theta
\end{align}
$$

::: {.nudge-down-small}
- Suppose we observe $\mathcal Y= \{\mathbf y_1,\dots,\mathbf y_n\}$. ML estimation maximises (up to a constant) the log-likelihood
  $$
  \ell(\vartheta)
  = -\frac{n}{2}\Bigl[
  \log \bigl|\boldsymbol\Sigma(\vartheta)\bigr|
  + \operatorname{tr} \bigl(\boldsymbol\Sigma(\vartheta)^{-1} \mathbf S\bigr)
  + \bigl(\bar {\mathbf y} - \boldsymbol\mu(\vartheta)\bigr)^{\top}
    \boldsymbol\Sigma(\vartheta)^{-1}
    \bigl(\bar {\mathbf y} - \boldsymbol\mu(\vartheta)\bigr)
  \Bigr]
  $$
  where 
    - $\mathbf S = \frac{1}{n}\sum_{i=1}^n (\mathbf y_i - \bar{\mathbf y})(\mathbf y_i - \bar{\mathbf y})^\top$ is the (biased) sample covariance matrix; and
    - $\bar{\mathbf y} = \frac{1}{n}\sum_{i=1}^n \mathbf y_i$ is the sample mean.

::: {.nudge-down-small}
- Clearly, the MLE aims to minimise the discrepancy between $\mathbf S$ and $\boldsymbol\Sigma(\vartheta)$.
:::

:::

## Properties of MLE

Let $\bar{\vartheta}$ be the true parameter value.
Subject to standard regularity conditions [@cox1979theoretical], as $n\to\infty$,

$$
\sqrt n (\hat\vartheta - \bar\vartheta) \xrightarrow{\;\;\text D\;\;} 
\begin{cases}
\text N_m\left(\mathbf 0, \big[ U(\bar\vartheta)V(\bar\vartheta)^{-1} U(\bar\vartheta) \big]^{-1} \right) &\text{model misspecified} \\
\text{N}_m\big(\mathbf 0, I(\bar\vartheta)^{-1} \big) &\text{otherwise}
\end{cases}
$$
where

- $I(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *Fisher information*;
- $U(\vartheta) = -\mathbb{E}\left[ \nabla\nabla^\top \ell_1(\vartheta) \right]$ is the *sensitivity matrix*; and
- $V(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *variability matrix*.

::: {.nudge-up}
Calculation of SEs are based off estimates of these matrices.
The "sandwich" matrix gives robust SEs [@satorra1994corrections;@savalei2014understanding].
:::


## Latent growth curve model (GCM)

- **Longitudinal data:** repeated measurements on individuals $i$ over time, e.g. $\mathbf y_i = (y_{i1},y_{i2},\dots,y_{i10})$, $i=1,\dots,n$ [@rabe2008multilevel].

- Usually, linear mixed effects models are used, where
$$
\begin{gathered}
y_{it} = \overbrace{(\alpha_1 + \eta_{1i})}^{\text{random int.}} +  \overbrace{(\alpha_2 + \eta_{2i})}^{\text{random slope}} \cdot (t-1) + \epsilon_{it} \quad\quad t=1,\dots,10 \\[0.5em]
\begin{pmatrix}
\eta_{1i} \\
\eta_{2i}
\end{pmatrix}
\sim \text{N}_2\left(\mathbf 0,
\begin{pmatrix}
\psi_{11} & \psi_{12} \\
\cdot & \psi_{22}
\end{pmatrix}\right) \\
\end{gathered}
$$

  ::: {.nudge-down-small}

  - $\alpha_1$ and $\alpha_2$ are fixed effects (intercept and slope);
  - $\eta_1$ and $\eta_2$ are correlated random effects (individual deviations);
  - $\epsilon_t\sim\text{N}(0,\theta)$ are measurement errors.

  :::

::: {.nudge-down-small}
- Restricted ML is a popular method to estimate such models, with good parameter recovery for variance components [@corbeil1976restricted].
:::

## Latent GCM as SEM

```{r, engine = "tikz"}
#| out-width: 85%
\usetikzlibrary{shapes.geometric,arrows,calc,positioning}
\begin{tikzpicture}[%
  >=stealth,              % for arrow tips
  auto,                   % automatic label positioning
  node distance=1.6cm,
  every node/.style={font=\small},
  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},
  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},
  error/.style={circle, draw, minimum size=6mm, inner sep=0mm},
intercept/.style={regular polygon, regular polygon sides=3, draw, inner sep=0pt, minimum size=6mm}
  ]

%-------------------------------------------------------
% 1) LATENT FACTORS (intercept i, slope s)
%-------------------------------------------------------
\node[latent] (i) at (-2,-3) {$\eta_{1}$};
\node[latent] (s) at ( 2,-3) {$\eta_{2}$};
\node[intercept] (int1) at (-2,-4.2) {\footnotesize 1};
\node[intercept] (int2) at (2,-4.2) {\footnotesize 1};

%-------------------------------------------------------
% 2) COVARIANCES AMONG LATENT FACTORS
%    - psi_{11} on i
%    - psi_{22} on s
%    - psi_{21} between i and s
%-------------------------------------------------------
% i <-> s
\draw[<->] (i) to[out=-45, in=225,looseness=0.9] node[below] {$\psi_{12}$} (s);

% Variance of i
\draw[<->] (i) to[out=190, in=220, looseness=4] 
  node[left] {$\psi_{11}$} (i);

% Variance of s
\draw[<->] (s) to[out=-40, in=-10, looseness=4] 
  node[right] {$\psi_{22}$} (s);

\draw[->] (int1) -- node[right,pos=0.3] {$\alpha_1$} (i);
\draw[->] (int2) -- node[right,pos=0.3] {$\alpha_2$} (s);

%-------------------------------------------------------
% 3) OBSERVED VARIABLES (y1..y10) + ERRORS (eps1..eps10)
%    Arrange them in a horizontal row above i and s.
%-------------------------------------------------------
\foreach \j in {1,...,10} {
  % X-position for y_j (shift them so they are roughly centred)
  \pgfmathsetmacro{\x}{(\j*1.3 - 6.5)}

  % Observed variable y_j
  \node[obs] (y\j) at (\x,0) {$y_{\j}$};

  % Error term epsilon_j above y_j
  \node[error] (e\j) at (\x,1.2) {$\epsilon_{\j}$};

  % Arrow from error to observed
  \draw[->] (e\j) -- (y\j);

  % Intercept factor loadings: all = 1
  \draw[->] (i) -- node[pos=0.45,right] {\footnotesize 1} (y\j);

  % Slope factor loadings: 0..9
  \pgfmathtruncatemacro{\loading}{\j - 1}
  \draw[->] (s) -- node[pos=0.7,right] {\footnotesize\loading} (y\j);
}

\foreach \i in {1,...,10}{
  \draw[<->] (e\i)
    to[out=110, in=140, looseness=5]
    node[above] {$\theta$}
    (e\i);
}

\end{tikzpicture}
```



::: {.absolute bottom=2% right=1%}

$$
\begin{gathered}
\boldsymbol\Lambda = \begin{bmatrix}
1 & 0 \\
1 & 1 \\
\vdots & \vdots \\
1 & 9 \\
\end{bmatrix} 
\\
\\
\mathbf y = \mathbf 0 + \boldsymbol\Lambda \boldsymbol\eta + \boldsymbol\epsilon \\
\boldsymbol\epsilon \sim \operatorname{N}_{10}(\mathbf 0, \theta \mathbf I) \\
\boldsymbol\eta \sim \operatorname{N}_2 (\boldsymbol\alpha, \boldsymbol\Psi) 
\end{gathered}
$$
:::





# Bias reduction methods {.transition-slide}

## What is bias?

::: {.callout-tip icon=false}
#### Bias of an estimator
$$
\mathcal B_{\bar\vartheta}(\hat\vartheta) = \mathbb E\left[\hat\vartheta - \bar\vartheta\right] 
$${#eq-bias}
:::


Consider the stochastic Taylor expansion of $s(\hat\vartheta)=\nabla\ell(\vartheta)=0$ around $\bar\vartheta$.
For many common estimators including MLE, the bias function is:
$$
\mathcal B_{\bar\vartheta} = \frac{b_1(\bar\vartheta)}{n} +  \frac{b_2(\bar\vartheta)}{n^2} + \frac{b_3(\bar\vartheta)}{n^3} +O(n^{-4}).
$$

::: {.nudge-up}
Bias arises because the roots of the score equations are **not exactly centred at $\bar\vartheta$**, due to:

::: {.nudge-up-small}
a. The curvature of the score $s(\vartheta)$ creating asymmetry; and
b. The randomness of the score itself.
:::
:::


## Illustration

### Biased MLE estimator for $\sigma^2$ 

Consider $X_1,\dots,X_n \sim \text N(0, \sigma^2)$. The MLE for $\sigma^2$ is $\hat\sigma^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 5)
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 25)
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 1000)
```

:::

## Illustration (cont.)

### Score functions are random too

The score is $s(\sigma^2)=\ell'(\sigma^2) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_score(n = 15) + coord_cartesian(ylim = c(-8, 28))
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 50, showbias = FALSE) + coord_cartesian(ylim = c(-8, 300))
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 1000, showbias = FALSE)
```

:::

## If you're interested... 

### ...and love differentiation ️❤️🤓

For a comprehensive treatment of bias-reduction methods,

- Start here: @cox1968general
- Follow up with: @firth1993bias; @kosmidis2009bias; @kosmidis2014bias


::: {.fragment}
::: {.nudge-up}
By the way, the $O(n^{-1})$ bias term $b_1(\bar\vartheta)/n = -I(\bar\vartheta)^{-1} C(\bar\vartheta)$, where
$$
\begin{gathered}
C_a(\vartheta) = \frac{1}{2} \operatorname{tr} \left[
I(\vartheta)^{-1}\big( G_a(\vartheta) + H_a(\vartheta) \big)
\right]\\ 
G_a(\vartheta)=\mathbb E[s(\vartheta)s(\vartheta)^\top s_a(\vartheta)] 
\quad\quad 
H_a(\vartheta)=-\mathbb E[I(\vartheta)s_a(\vartheta)] \\
a=1,\dots,m
\end{gathered}
$$
where $s(\vartheta) = \nabla\ell(\vartheta)$ is the score function.
:::
:::

## A review

::: {.nudge-up-large}
![](figures/bias.png){fig-align=center width=67%}
:::

::: {.nudge-up-small}
::: {.nudge-up-large}
```{r}
#| html-table-processing: none
library(gt)
library(dplyr)

df <- tribble(
  ~Method, ~Model, ~BG_theta0, ~Type, ~E, ~d, ~theta_hat,
  "Asymptotic bias correction", "full", "analytical", "explicit", "✓","✓","✓",
  "Adjusted score functions", "full", "analytical", "implicit", "✓","✓","✗",
  "Bootstrap", "partial", "simulation", "explicit", "✗","✗","✓",
  "Jackknife", "partial", "simulation", "explicit", "✗","✗","✓",
  "Indirect inference", "full", "simulation", "implicit", "✗","✗","✓",
  "Explicit RBM", "partial", "analytical", "explicit", "✗","✓","✓",
  "Implicit RBM", "partial", "analytical", "implicit", "✗","✓","✗"
)

gt(df, rownames_to_stub = TRUE) |>
  text_transform(
    locations = cells_body(columns = c(E, d, theta_hat)),
    fn = \(x) {
      x |>
        str_replace("✓", '<span style="color:#004C59">✓</span>') |>
        str_replace("✗", '<span style="color:#b10f2e">✗</span>')
      # gt::html(out) 
  }) |>
  tab_spanner(md("**Requirements**"), columns = c(E, d, theta_hat)) |>
  cols_label(
    BG_theta0 = md("$\\mathcal{B}(\\bar\\vartheta)$"),
    E = md("$\\mathbb{E}(\\cdot)$"),
    d = md("$\\hspace{2pt} \\partial \\cdot \\hspace{2pt}$"),
    theta_hat = md("$\\hspace{4pt} \\hat\\vartheta \\hspace{4pt}$")
  ) |>
  cols_align(
    align = "center",
    columns = c(E, d, theta_hat)
  ) |>
  tab_style(
    style = "font-weight: bold",
    locations = cells_column_labels()
  ) |>
  tab_options(
    table.font.size = px(25),
    table.width = pct(95)
    # data_row.padding = px(4)
  ) |>
  opt_table_font(
    font = list(
      google_font("Raleway"),  # loads the font for the table
      default_fonts()          # sensible fallbacks
    )
  )
```
:::
:::

::: aside
::: {.footnotesize-text}
1--@efron1975defining, @cordeiro1991bias; 2--@firth1993bias, @kosmidis2009bias; 3--@efron1994introduction, @hall1988bootstrap; 4--@quenouille1956notes, @efron1982jackknife; 5--@gourieroux1993indirect, @mackinnon1998approximate
:::
:::

## Firth's adjusted scores methods

::: {.nudge-up}
Instead of solving [$s(\vartheta)=0$]{.text-org}, solve [$s(\vartheta) + \overbrace{A(\vartheta)}^{\mathcal B(\vartheta) I(\vartheta)}=0$]{.text-tur}.
:::

<!-- ```{r} -->
<!-- #| out-width: 100% -->
<!-- #| fig-align: center -->
<!-- #| fig-height: 4 -->
<!-- #| fig-width: 8 -->
<!-- plot_adjusted_score() -->
<!-- ``` -->

::: {.nudge-up}
![](figures/adjusted_score.gif){fig-align=center width=100%}
:::

## Implicit RBM estimator

Computing $\mathcal B(\vartheta)$ and $I(\vartheta)$ can be difficult. 
Consider

$$
s(\vartheta) + A(\vartheta) = 0 \ \Leftrightarrow \ \underset{\vartheta}{\text{arg max}} \left \{ \ell(\vartheta) + P(\vartheta) \right\}
$${#eq-irbm}

where $P(\vartheta)$ is a penalty term constructed such that $A(\vartheta) = \nabla P(\vartheta)$. 
@kosmidis2024empirical show that
$$
P(\vartheta) = -\frac{1}{2} \operatorname{tr} \Big\{ j(\vartheta)^{-1} e(\vartheta) \Big\}
$$ 

where

::: {.nudge-up-small}
- $j(\vartheta) = \sum_{i=1}^n \nabla\nabla^\top \ell_i(\vartheta)$ is the observed information;
- $e(\vartheta) = \sum_{i=1}^n \nabla\ell_i(\vartheta)\nabla^\top\ell_i(\vartheta)\nabla\ell_i(\vartheta)$ is the outer-product of scores.

::: {.nudge-up-small}
The solution $\tilde\vartheta$ to (-@eq-irbm) is called the [*implicit* RBM estimator (iRBM)]{}.
:::
:::

## Explicit RBM estimator

Intuitively, by thinking in terms of a Newton-style update, an *explicit* estimator is obtained via
$$
\vartheta^*= \hat\vartheta + j(\hat\vartheta)^{-1} A(\hat\vartheta).
$$

This moves $\hat\theta$ in the direction $A(\hat\vartheta)$ away from the bias, with step length governed by the curvature $j(\hat\vartheta)^{-1}$.

::: {.columns}

::: {.column width=50%}
{{< placeholder 600 300 format=svg >}}
:::

::: {.column width=50%}
- Operationally, eRBM is simpler and quicker to compute than iRBM---no re-optimisation needed if $\hat\theta$ is available.

- However, unlike iRBM, no guarantees that bias correction stays inside the parameter space.
:::

:::

<!-- ## Resampling-based methods -->

# Simulation studies {.transition-slide}

## Simulation design

<!-- - Data generating models: -->
<!--    1. Two-factor SEM ($m=13$ estimable params). -->
<!--    2. Latent growth curve model ($m=6$ estimable params). -->
   
- Sample size: $n\in \{15,20,50,100,1000\}$

- Item reliability: Low or High ($\operatorname{Rel} = p^{-1}\sum_{j=1}^p \boldsymbol\Sigma_{jj}^* / \boldsymbol\Sigma_{jj}$)

- Distributional assumption: Normal or Non-normal

```{r}
#| fig-height: 3.6
#| fig-width: 9
#| fig-align: center
#| out-width: 100%
n <- 1000
rho <- 0.3
Sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
dat <- list()
dat$Normal <-
  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(0, 2))[[1]]
## dat$Kurtosis <-  # kurtosis
##   covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(6, 2))[[1]]
dat$`Non-normal` <-  # non-normal
  covsim::rIG(n, Sigma, skewness = rep(-2, 2), excesskurtosis = rep(6, 2))[[1]]

# get max and min for x and y axes
xyminmax <- 
  map(dat, as.data.frame) |>
  bind_rows() |>
  summarise(
    x_min = min(V1),
    x_max = max(V1),
    y_min = min(V2),
    y_max = max(V2)
  ) |> 
  unlist() 

allplots <- imap(dat, \(x, idx) {
  # mycols <- rev(RColorBrewer::brewer.pal(3, "Set1"))
  mycols <- c("#00A6AA",  "#F18F00", "black")
  names(mycols) <- names(dat)
  
  p <-
    as.data.frame(x) |>
    ggplot(aes(x = V1, y = V2)) +
    geom_point(alpha = 0.5, size = 1, col = mycols[idx]) +
    geom_density_2d(col = "black") +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text = element_blank(),
      axis.ticks = element_blank()
    ) +
    scale_x_continuous(limits = c(xyminmax["x_min"], xyminmax["x_max"])) +
    scale_y_continuous(limits = c(xyminmax["y_min"], xyminmax["y_max"])) +
    labs(title = glue::glue("{idx}"), x = NULL, y = NULL)
  ggExtra:::ggMarginal(p, fill = mycols[idx], alpha = 0.5, size = 10, 
                       trim = idx != "Non-normal") 
})
cowplot::plot_grid(plotlist = allplots, nrow = 1)
```


## Results: Two-factor SEM 

::: {.nudge-up-small}

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.5
#| fig-width: 9
p1
```

:::


## Results: Two-factor SEM (cont.)

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.4
#| fig-width: 9
p2
```

## Results: Latent GCM

::: {.nudge-up}

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.4
#| fig-width: 9
p3
```

:::

# Conclusion {.transition-slide}

## Summary & future work

RBM applied to small sample estimation of SEM show key advantages:

- Improved estimator performances (mean & median bias, RMSE, coverage).

- Computationally efficient (c.f. resampling methods).

- Robust to model misspecification.

Future work include

1. Computational improvements for iRBM.

2. Plugin penalties to limit exploration of ill-conditioned regions.

3. Extension to other SEM families, e.g.
   - Mediation models, latent interactions, etc.
   - Alternative to ML estimation e.g. WLS, DWLS, etc.

## Software 

```{r}
#| include: false
library(brlavaan)
dat <- brlavaan::gen_data_twofac(n = 50)
colnames(dat) <- paste0("y", 1:6)
options(width = 180)
```


```{r}
#| echo: true
#| cache: true
# pak::pak("haziqj/brlavaan")
library(brlavaan)

mod <- "
  eta1 =~ y1 + y2 + y3
  eta2 =~ y4 + y5 + y6
"
fit <- brsem(model = mod, data = dat, estimator.args = list(rbm = "implicit"))
summary(fit)
```

# شكراً جزيلاً {.thanks-slide  background-image="_extensions/haziqj/kaust/KAUST-Thank-you.jpg" style="padding-top:0.5em;padding-bottom:0em"}

[`https://haziqj.ml/sembias-gradsem`](https://haziqj.ml/sembias-gradsem)

## References