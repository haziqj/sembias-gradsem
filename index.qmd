---
title: 'Bias-reduced estimation of structural equation models'
# subtitle: A statistical perspective
format:
  kaust-revealjs:
    slide-level: 2
    transition: fade
    auto-stretch: false
    width: 1250  # 1050
    height: 760  # 700
    self-contained: false
    chalkboard: true
    toc: false
    toc-depth: 1
    # multiplex: true
    code-block-height: 700px
    # html-table-processing: none
author:
  - name: Haziq Jamil
    orcid: 0000-0003-3298-1010
    affiliations: 
      - 'Research Specialist, BAYESCOMP @ CEMSE-KAUST'
      - '<span style="font-style:normal;">[`https://haziqj.ml/sem-bias/`](https://haziqj.ml/sem-bias/)</span>'
date: 2025-10-16
bibliography: refs.bib
execute:
  echo: false
  freeze: auto
  cache: false
---

## 

```{r}
#| include: false
source("R/sigma2.R")
source("R/score.R")
source("R/adjscore.R")
```


::: {.columns}

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://lavaan.org" target="_blank">
    <img
      src="https://science-academy.ugent.be/sites/acugain/files/styles/medium_1_1/public/teachers/rosseel_yves.jpg?h=04d92ac6&itok=GUYp9IIm"
      alt="Yves Rosseel"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>
    Yves Rosseel<br><em>Universiteit Gent</em> | R/<code>{lavaan}</code>
  </figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <img
    src="https://warwick.ac.uk/fac/sci/statistics/staff/research_students/kemp/img_warwick.jpg"
    style="--size:220px; width:var(--size); height:var(--size);
           object-fit:cover; object-position:top;"
  >
  <figcaption>Ollie Kemp<br><em>University of Warwick</em></figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://www.ikosmidis.com" target="_blank">
    <img
      src="https://www.ikosmidis.com/img/me.jpg"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>Ioannis Kosmidis<br><em>University of Warwick</em></figcaption>
</figure>
:::

:::

::: {.nudge-up}

> **Jamil, H.**, Rosseel, Y., Kemp, O., & Kosmidis, I. (2025). Bias-Reduced Estimation of Structural Equation Models. *Manuscript in Submission*. [`arXiv:2509.25419`](https://doi.org/10.48550/arXiv.2509.25419).

- Source: <https://github.com/haziqj/sembias-gradsem>
- R Package: <https://github.com/haziqj/brlavaan>

:::

[{{< placeholder 220 220 format=svg >}}]{.absolute bottom=10 right=0}

[[**poll**]{.bg-grn}]{.absolute bottom=220 right=235}

## Context

::: {.callout-note icon=false title="SEM in a nutshell"}
Analyse multivariate data $\mathbf y=(y_1,\dots,y_p)^\top$ to measure and relate hidden variables $\boldsymbol\eta=(\eta_1,\dots,\eta_q)^\top$, $q \ll p$, and uncover complex patterns.
:::

::: {.nudge-up-small}
In the social sciences, latent variables are used to represent **constructs**---the *theoretical, unobserved* concepts of interest.
:::

::: {.nudge-up}
:::: {.columns}

::: {.column width="25%"}
![*(Psychology)*<br>Personality traits](https://unsplash.com/photos/5bYxXawHOQg/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzMTM4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Healthcare)*<br>Quality of life](https://unsplash.com/photos/hIgeoQjS_iE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjI2Nzg4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Political science)*<br>Social trust](https://unsplash.com/photos/zjeZXMU1SKE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzIxfA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Education)*<br>Competencies](https://unsplash.com/photos/oXV3bzR7jxI/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzM1fA&force=true&w=640){height=210px}
:::

::::
:::

::: {.aside}
Photo credits: Unsplash
[\@dtravisphd](https://unsplash.com/photos/brown-fountain-pen-on-notebook-5bYxXawHOQg),
[\@impulsq](https://unsplash.com/photos/doctor-holding-red-stethoscope-hIgeoQjS_iE),
[\@ev](https://unsplash.com/photos/a-group-of-police-standing-next-to-each-other-zjeZXMU1SKE),
[\@benmullins](https://unsplash.com/photos/person-using-pencil-oXV3bzR7jxI).
:::

## Motivation

*"Using SEMs in empirical research is often challenged by small sample sizes."*

- Why? Data collection is expensive, time-consuming, or difficult.
- Rare populations:
    - **@quezada2016explanatory:** Identifying factors of adjustment in pediatric burn patients to facilitate appropriate mental health interventions postinjury ($n=51$).
    - **@figueroa2021structural:** Studying functional connectivity network on individuals with rare genetic disorders ($n=22$).
    - **@fabbricatore2023componentbased**: Assessment of psycho-social aspects and performance of elite swimmers ($n=161$).
    - **@manuela2013pacific**: Validating self-report measures of identity on a unique cultural group ($n=143$).
- SEM is desirable, but small $n \Rightarrow$ poor finite-sample performance (esp. bias). 

## Outline

1. Brief overview of SEMs
    - Model equations
    - ML estimation and inference
    - Examples of SEMs
      - Two-factor SEM
      - Latent growth models
2. Bias reducing methods
    - A review
    - Resampling-based methods
    - Alternative approaches
    - Reduced-Bias $M$-estimation (RBM)
3. Simulation studies and results

# Structural equation models {.transition-slide}

## Measurement model

## Structural model

## Example 1: Political democracy example

## ML estimation

## Properties of MLE

Let $\bar{\vartheta}$ be the true parameter value.
Subject to standard regularity conditions [@cox1979theoretical], as $n\to\infty$,

$$
\sqrt n (\hat\vartheta - \bar\vartheta) \xrightarrow{\;\;\text D\;\;} 
\begin{cases}
\text N_m\left(\mathbf 0, \big[ U(\bar\vartheta)V(\bar\vartheta)^{-1} U(\bar\vartheta) \big]^{-1} \right) &\text{model misspecified} \\
\text{N}_m\big(\mathbf 0, I(\bar\vartheta)^{-1} \big) &\text{otherwise}
\end{cases}
$$
where

- $I(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *Fisher information*;
- $U(\vartheta) = -\mathbb{E}\left[ \nabla\nabla^\top \ell_1(\vartheta) \right]$ is the *sensitivity matrix*; and
- $V(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *variability matrix*.

::: {.nudge-up}
Calculation of SEs are based off estimates of these matrices.
The "sandwich" matrix gives robust SEs [@satorra1994corrections;@savalei2014understanding].
:::


## Example 2: Latent growth curve model

## REML

# Bias reduction methods {.transition-slide}

## What is bias?

::: {.callout-tip icon=false}
#### Bias of an estimator
$$
\mathcal B_{\bar\vartheta}(\hat\vartheta) = \mathbb E\left[\hat\vartheta - \bar\vartheta\right] 
$${#eq-bias}
:::


::: {.nudge-up}
Consider the stochastic Taylor expansion of $s(\hat\vartheta)=\nabla\ell(\vartheta)=0$ around $\bar\vartheta$.
For many common estimators including MLE, the bias function is:
$$
\mathcal B_{\bar\vartheta} = \frac{b_1(\bar\vartheta)}{n} +  \frac{b_2(\bar\vartheta)}{n^2} + \frac{b_3(\bar\vartheta)}{n^3} +O(n^{-4}).
$$

::: {.nudge-up}
Bias arises because the roots of the score equations are **not exactly centred at $\bar\vartheta$**, due to:

::: {.nudge-up-small}
a. The curvature of the score $s(\vartheta)$; and
b. The randomness of the score itself.
:::
:::
:::

## Illustration

### Biased MLE estimator for $\sigma^2$ 

Consider $X_1,\dots,X_n \sim \text N(0, \sigma^2)$. The MLE for $\sigma^2$ is $\hat\sigma^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 5)
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 25)
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 1000)
```

:::

## Illustration (cont.)

### Score functions are random too

The score is $s(\sigma^2)=\ell'(\sigma^2) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_score(n = 15) + coord_cartesian(ylim = c(-8, 28))
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 50, showbias = FALSE) + coord_cartesian(ylim = c(-8, 300))
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 1000, showbias = FALSE)
```

:::

## If you're interested... 

### ...and love differentiation Ô∏è‚ù§Ô∏èü§ì

For a comprehensive treatment of bias-reduction methods,

- Start here: @cox1968general
- Follow up with: @firth1993bias; @kosmidis2009bias; @kosmidis2014bias


::: {.fragment}
::: {.nudge-up}
By the way, the $O(n^{-1})$ bias term $b_1(\bar\vartheta)/n = -I(\bar\vartheta)^{-1} C(\bar\vartheta)$, where
$$
\begin{gathered}
C_a(\vartheta) = \frac{1}{2} \operatorname{tr} \left[
I(\vartheta)^{-1}\big( G_a(\vartheta) + H_a(\vartheta) \big)
\right]\\ 
G_a(\vartheta)=\mathbb E[s(\vartheta)s(\vartheta)^\top s_a(\vartheta)] 
\quad\quad 
H_a(\vartheta)=-\mathbb E[I(\vartheta)s_a(\vartheta)] \\
a=1,\dots,m
\end{gathered}
$$
where $s(\vartheta) = \nabla\ell(\vartheta)$ is the score function.
:::
:::

## A review

::: {.nudge-up-large}
![](figures/bias.png){fig-align=center width=67%}
:::

::: {.nudge-up-small}
::: {.nudge-up-large}
```{r}
#| html-table-processing: none
library(gt)
library(dplyr)

df <- tribble(
  ~Method, ~Model, ~BG_theta0, ~Type, ~E, ~d, ~theta_hat,
  "Asymptotic bias correction", "full", "analytical", "explicit", "‚úì","‚úì","‚úì",
  "Adjusted score functions", "full", "analytical", "implicit", "‚úì","‚úì","‚úó",
  "Bootstrap", "partial", "simulation", "explicit", "‚úó","‚úó","‚úì",
  "Jackknife", "partial", "simulation", "explicit", "‚úó","‚úó","‚úì",
  "Indirect inference", "full", "simulation", "implicit", "‚úó","‚úó","‚úì",
  "Explicit RBM", "partial", "analytical", "explicit", "‚úó","‚úì","‚úì",
  "Implicit RBM", "partial", "analytical", "implicit", "‚úó","‚úì","‚úó"
)

gt(df, rownames_to_stub = TRUE) |>
  text_transform(
    locations = cells_body(columns = c(E, d, theta_hat)),
    fn = \(x) {
      x |>
        str_replace("‚úì", '<span style="color:#004C59">‚úì</span>') |>
        str_replace("‚úó", '<span style="color:#b10f2e">‚úó</span>')
      # gt::html(out) 
  }) |>
  tab_spanner(md("**Requirements**"), columns = c(E, d, theta_hat)) |>
  cols_label(
    BG_theta0 = md("$\\mathcal{B}(\\bar\\vartheta)$"),
    E = md("$\\mathbb{E}(\\cdot)$"),
    d = md("$\\hspace{2pt} \\partial \\cdot \\hspace{2pt}$"),
    theta_hat = md("$\\hspace{4pt} \\hat\\vartheta \\hspace{4pt}$")
  ) |>
  cols_align(
    align = "center",
    columns = c(E, d, theta_hat)
  ) |>
  tab_style(
    style = "font-weight: bold",
    locations = cells_column_labels()
  ) |>
  tab_options(
    table.font.size = px(25),
    table.width = pct(95)
    # data_row.padding = px(4)
  ) |>
  opt_table_font(
    font = list(
      google_font("Raleway"),  # loads the font for the table
      default_fonts()          # sensible fallbacks
    )
  )
```
:::
:::

::: aside
::: {.footnotesize-text}
1--@efron1975defining, @cordeiro1991bias; 2--@firth1993bias, @kosmidis2009bias; 3--@efron1994introduction, @hall1988bootstrap; 4--@quenouille1956notes, @efron1982jackknife; 5--@gourieroux1993indirect, @mackinnon1998approximate
:::
:::

## Firth's adjusted scores methods

Instead of solving [$s(\vartheta)=0$]{.text-org}, solve [$s(\vartheta) + \mathcal B(\vartheta) I(\vartheta)=0$]{.text-tur}.

<!-- ```{r} -->
<!-- #| out-width: 100% -->
<!-- #| fig-align: center -->
<!-- #| fig-height: 4 -->
<!-- #| fig-width: 8 -->
<!-- plot_adjusted_score() -->
<!-- ``` -->

![](figures/adjusted_score.gif){fig-align=center width=100%}

## RBM framework

## Resampling-based methods

# Simulation Studies {.transition-slide}

## Simulation design

## Results: Two-factor SEM 

## Results: Latent GCM

# Conclusion {.transition-slide}

## Summary & future work

## Software


# ÿ¥ŸÉÿ±ÿßŸã ÿ¨ÿ≤ŸäŸÑÿßŸã {.thanks-slide  background-image="_extensions/haziqj/kaust/KAUST-Thank-you.jpg" style="padding-top:0.5em;padding-bottom:0em"}

[`https://haziqj.ml/sembias-gradsem`](https://haziqj.ml/sembias-gradsem)

## References