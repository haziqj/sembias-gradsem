---
title: 'Bias-reduced estimation of structural equation models'
# subtitle: A statistical perspective
format:
  kaust-revealjs:
    slide-level: 2
    transition: fade
    auto-stretch: false
    width: 1250  # 1050
    height: 760  # 700
    self-contained: false
    chalkboard: true
    toc: false
    toc-depth: 1
    # multiplex: true
    code-block-height: 700px
    # html-table-processing: none
author:
  - name: Haziq Jamil
    orcid: 0000-0003-3298-1010
    affiliations: 
      - 'Research Specialist, BAYESCOMP @ CEMSE-KAUST'
      - '<span style="font-style:normal;">[`https://haziqj.ml/sem-bias/`](https://haziqj.ml/sem-bias/)</span>'
date: 2025-10-16
bibliography: refs.bib
execute:
  echo: false
  freeze: auto
  cache: true
---

## 

```{r}
#| include: false
source("R/sigma2.R")
source("R/score.R")
source("R/adjscore.R")
source("R/results.R")
```


::: {.columns}

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://lavaan.org" target="_blank">
    <img
      src="https://science-academy.ugent.be/sites/acugain/files/styles/medium_1_1/public/teachers/rosseel_yves.jpg?h=04d92ac6&itok=GUYp9IIm"
      alt="Yves Rosseel"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>
    Yves Rosseel<br><em>Universiteit Gent</em> | R/<code>{lavaan}</code>
  </figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <img
    src="https://warwick.ac.uk/fac/sci/statistics/staff/research_students/kemp/img_warwick.jpg"
    style="--size:220px; width:var(--size); height:var(--size);
           object-fit:cover; object-position:top;"
  >
  <figcaption>Ollie Kemp<br><em>University of Warwick</em></figcaption>
</figure>
:::

::: {.column width="33%"}
<figure class="quarto-figure quarto-figure-center" style="text-align:center;">
  <a href="https://www.ikosmidis.com" target="_blank">
    <img
      src="https://www.ikosmidis.com/img/me.jpg"
      style="--size:220px; width:var(--size); height:var(--size);
             object-fit:cover; object-position:top;"
    >
  </a>
  <figcaption>Ioannis Kosmidis<br><em>University of Warwick</em></figcaption>
</figure>
:::

:::

::: {.nudge-up}

> **Jamil, H.**, Rosseel, Y., Kemp, O., & Kosmidis, I. (2025). Bias-Reduced Estimation of Structural Equation Models. *Manuscript in Submission*. [`arXiv:2509.25419`](https://doi.org/10.48550/arXiv.2509.25419).

- Source: <https://github.com/haziqj/sembias-gradsem>
- R Package: <https://github.com/haziqj/brlavaan>

:::

[{{< placeholder 220 220 format=svg >}}]{.absolute bottom=10 right=0}

[[**poll**]{.bg-grn}]{.absolute bottom=220 right=235}

## Context

::: {.callout-note icon=false title="SEM in a nutshell"}
Analyse multivariate data $\mathbf y=(y_1,\dots,y_p)^\top$ to measure and relate hidden variables $\boldsymbol\eta=(\eta_1,\dots,\eta_q)^\top$, $q \ll p$, and uncover complex patterns.
:::

::: {.nudge-up-small}
In the social sciences, latent variables are used to represent **constructs**---the *theoretical, unobserved* concepts of interest.
:::

::: {.nudge-up}
:::: {.columns}

::: {.column width="25%"}
![*(Psychology)*<br>Personality traits](https://unsplash.com/photos/5bYxXawHOQg/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzMTM4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Healthcare)*<br>Quality of life](https://unsplash.com/photos/hIgeoQjS_iE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjI2Nzg4fA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Political science)*<br>Social trust](https://unsplash.com/photos/zjeZXMU1SKE/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzIxfA&force=true&w=640){height=210px}
:::

::: {.column width="25%"}
![*(Education)*<br>Competencies](https://unsplash.com/photos/oXV3bzR7jxI/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzU5MjMzNzM1fA&force=true&w=640){height=210px}
:::

::::
:::

::: {.aside}
Photo credits: Unsplash
[\@dtravisphd](https://unsplash.com/photos/brown-fountain-pen-on-notebook-5bYxXawHOQg),
[\@impulsq](https://unsplash.com/photos/doctor-holding-red-stethoscope-hIgeoQjS_iE),
[\@ev](https://unsplash.com/photos/a-group-of-police-standing-next-to-each-other-zjeZXMU1SKE),
[\@benmullins](https://unsplash.com/photos/person-using-pencil-oXV3bzR7jxI).
:::

## Motivation

*"Using SEMs in empirical research is often challenged by small sample sizes."*

- Why? Data collection is expensive, time-consuming, or difficult.
- Rare populations:
    - **@quezada2016explanatory:** Identifying factors of adjustment in pediatric burn patients to facilitate appropriate mental health interventions postinjury ($n=51$).
    - **@figueroa2021structural:** Studying functional connectivity network on individuals with rare genetic disorders ($n=22$).
    - **@fabbricatore2023componentbased**: Assessment of psycho-social aspects and performance of elite swimmers ($n=161$).
    - **@manuela2013pacific**: Validating self-report measures of identity on a unique cultural group ($n=143$).
- SEM is desirable, but small $n \Rightarrow$ poor finite-sample performance (esp. bias). 

## Outline

1. **Brief overview of SEMs**
    - Motivating example
    - ML estimation and inference
    - Examples of SEMs
      - Two-factor SEM
      - Latent growth models
2. **Bias reducing methods**
    - What is bias?
    - A review of bias reduction methods
    - Reduced-Bias $M$-estimation (RBM)
      - Implicit correction
      - Explicit correction
3. **Simulation studies and results**

# Structural equation models {.transition-slide}

## Motivating example

### Glycemic control and kidney health

> Does poorer glycemic control lead to greater severity of kidney disease?


::: {.columns}

::: {.column width=48%}
For $i=1,\dots,n$ patients, observe $p=6$ variables:

```{r}
#| html-table-processing: none
library(gt)

tbl <- data.frame(
  Variable = c("$x_1$", "$x_2$", "$x_3$", "$y_1$", "$y_2$", "$y_3$"),
  Indicator = c("HbA1c", "FPG", "Insulin", "PCr", "ACR", "BUN"),
  Description = c(
    "3-month avg. blood glucose",
    "Fasting plasma glucose",
    "Fasting insulin level",
    "Plasma creatinine",
    "Albumin‚Äìcreatinine ratio",
    "Blood urea nitrogen"
  ),
  Unit = c("%", "mmol/L", "¬µU/mL", "¬µmol/L", "mg/g", "mmol/L"),
  stringsAsFactors = FALSE
)

tbl |>
  gt() |>
  # tab_header(title = md("**Observed Variables**")) |>
  cols_label(
    Variable   = "Var.",
    Indicator  = "Indicator",
    Description = "Description",
    Unit       = "Unit"
  ) |>
  fmt_markdown(columns = "Variable") |>
  tab_options(
    table.font.size = px(25),
    table.width = pct(100)
  ) |>  
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) |>
  opt_table_font(font = list(google_font("Raleway"), default_fonts()))
```

:::

::: {.column width=2%}
:::

::: {.column width=50%}
::: {.nudge-down}
"*casewise*" thinking leads to
$$
\small
\begin{align*}
y_{1} &= \beta_0^{(1)} + \beta_1^{(1)} x_1 + \beta_2^{(1)} x_2 + \beta_3^{(1)} x_3 + \epsilon^{(j)} \\
y_{2} &= \beta_0^{(2)} + \beta_1^{(2)} x_1 + \beta_2^{(2)} x_2 + \beta_3^{(2)} x_3 + \epsilon^{(j)} \\
y_{3} &= \beta_0^{(3)} + \beta_1^{(3)} x_1 + \beta_2^{(3)} x_2 + \beta_3^{(3)} x_3 + \epsilon^{(j)} \\
\end{align*}
$$

- Does not give a clear and direct answer.

- Moreover, $x$'s are assumed to be measured without error.

:::
:::

:::




::: aside
Example adapted from @song2012basic.
:::

## Covariance-based approach

::: {.columns}

::: {.column width=45%}
::: {.nudge-up-small}
Sample correlation matrix looks like this^[Simulated data, from a two-factor SEM ($n=1000$).]:

```{r}
#| fig-align: center
#| out-width: 100%
#| fig-height: 4.5
#| fig-width: 4.5
library(brlavaan)
dat <- gen_data_twofac(n = 1000)
z <- cor(dat)
colnames(z) <- rownames(z) <- c("x[1]","x[2]","x[3]","y[1]","y[2]","y[3]")

library(ggcorrplot)
ggcorrplot(
  z[, 6:1],
  lab = TRUE, lab_col = "black", lab_size = 4,
  colors = c("#00A6AA", "white", "#F18F00"),
  type = "full",    
  show.diag = TRUE
) +
  scale_x_discrete(position = "top", labels = scales::label_parse()) +
  scale_y_discrete(labels = scales::label_parse()) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 0.5),
    axis.ticks = element_blank(),
    legend.position = "none"
  )
```
:::
:::

::: {.column width=55%}
::: {.nudge-down-medium}
- The data suggests clustering of variables
   - $x$'s measure *glycemic control*
   - $y$'s measure *kidney health*

- There is an element of dimension-reduction, much needed for analysing (correlated) multivariate data.

- Easier to hypothesize relationships, e.g.
  $$
  \texttt{KdnHlt} = \alpha + \beta \,  \texttt{GlyCon} + \texttt{error}
  $$


- SEM is about modelling the <u>covariance structure</u> of the data, 
$$
\boldsymbol\Sigma = \boldsymbol\Sigma(\vartheta).
$$
:::
:::

:::

## SEM equations

::: {.columns}

::: {.column width=50%}
::: {.nudge-up}
$$
\small
\begin{gathered}
\begin{pmatrix}
y_1 \\
y_2 \\
y_3 \\
y_4 \\
y_5 \\
y_6
\end{pmatrix}
= 
\begin{pmatrix}
\lambda_{11} & 0 \\
\lambda_{21} & 0 \\
\lambda_{31} & 0 \\
0 & \lambda_{42} \\
0 & \lambda_{52} \\
0 & \lambda_{62} \\
\end{pmatrix}
\begin{pmatrix}
\eta_1 \\
\eta_2
\end{pmatrix}
+ 
\begin{pmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\epsilon_5 \\
\epsilon_6
\end{pmatrix} \\[1em]
\begin{pmatrix}
\eta_1 \\
\eta_2
\end{pmatrix}
=
\begin{pmatrix}
0 & 0 \\
\beta & 0 \\
\end{pmatrix}
\begin{pmatrix}
\eta_1 \\
\eta_2
\end{pmatrix}
+
\begin{pmatrix}
\zeta_1 \\
\zeta_2
\end{pmatrix}
\end{gathered}
$$

Or, more compactly as

::: {.nudge-up}
$$
\begin{gathered}
\mathbf y = {\color{Gray}\boldsymbol\nu +\,}  \boldsymbol\Lambda \boldsymbol\eta + \boldsymbol\epsilon \\
\boldsymbol\eta = {\color{Gray}\boldsymbol\alpha +\,} \mathbf B \boldsymbol\eta + \boldsymbol\zeta
\end{gathered}
$$

with assumptions $\boldsymbol\epsilon\sim\operatorname{N}_p(\mathbf 0, \boldsymbol\Theta)$, $\boldsymbol\zeta\sim\operatorname{N}_q(\mathbf 0, \boldsymbol\Psi)$, and $\operatorname{Cov}(\boldsymbol\epsilon, \boldsymbol\zeta)=\mathbf 0$.

:::



:::
:::

::: {.column width=50%}
```{r, engine = "tikz"}
#| fig-align: center
#| out-width: 100%
\begin{tikzpicture}[%
  scale=0.88,
  >=stealth,
  auto,
  node distance=1.6cm,
  every node/.style={font=\small},
  latent/.style={font=\normalsize,circle, draw, minimum size=9mm, inner sep=0mm},
  obs/.style={font=\normalsize,rectangle, draw, minimum size=6mm, inner sep=0mm},
  error/.style={font=\normalsize,circle, draw, minimum size=5mm, inner sep=0mm}
]

% --------------------------------------------------------
% 1) LATENT VARIABLES
% --------------------------------------------------------
\node[latent] (eta1) at (-1.5, -2) {$\eta_{1}$};
\node[latent] (eta2) at ( 3.5, -2) {$\eta_{2}$};

% --------------------------------------------------------
% 2) OBSERVED VARIABLES + ERROR TERMS
%    Place y1..y3 (for eta1) on the left, y4..y6 (for eta2) on the right.
% --------------------------------------------------------
\node[obs] (y1) at (-2.8, 0) {$y_{1}$};
\node[obs] (y2) at (-1.5, 0) {$y_{2}$};
\node[obs] (y3) at ( -0.2, 0) {$y_{3}$};

\node[obs] (y4) at ( 2.2, 0) {$y_{4}$};
\node[obs] (y5) at ( 3.5, 0) {$y_{5}$};
\node[obs] (y6) at ( 4.8, 0) {$y_{6}$};

% Error terms above each observed variable
\node[error] (e1) at (-2.8, 1) {$\epsilon_{1}$};
\node[error] (e2) at (-1.5, 1) {$\epsilon_{2}$};
\node[error] (e3) at ( -0.2, 1) {$\epsilon_{3}$};
\node[error] (e4) at ( 2.2, 1) {$\epsilon_{4}$};
\node[error] (e5) at ( 3.5, 1) {$\epsilon_{5}$};
\node[error] (e6) at ( 4.8, 1) {$\epsilon_{6}$};

% --------------------------------------------------------
% 3) PATHS FROM ERROR TERMS TO OBSERVED VARIABLES
% --------------------------------------------------------
\draw[->] (e1) -- (y1);
\draw[->] (e2) -- (y2);
\draw[->] (e3) -- (y3);
\draw[->] (e4) -- (y4);
\draw[->] (e5) -- (y5);
\draw[->] (e6) -- (y6);

% --------------------------------------------------------
% 4) FACTOR LOADINGS
%    \eta_{i1} -> y1, y2, y3
%    \eta_{i2} -> y4, y5, y6
% --------------------------------------------------------
\draw[->] (eta1) -- node[pos=0.5, right] {$1$} (y1);
\draw[->] (eta1) -- node[pos=0.5, right] {$\lambda_{21}$} (y2);
\draw[->] (eta1) -- node[pos=0.5, right] {$\lambda_{31}$} (y3);

\draw[->] (eta2) -- node[pos=0.5, right] {$1$} (y4);
\draw[->] (eta2) -- node[pos=0.5, right] {$\lambda_{52}$} (y5);
\draw[->] (eta2) -- node[pos=0.5, right] {$\lambda_{62}$} (y6);

% --------------------------------------------------------
% 5) REGRESSION BETWEEN LATENT VARIABLES
%    \eta_1 -> \eta_2, labeled beta
% --------------------------------------------------------
\draw[->] (eta1) -- node[midway, above] {$\beta$} (eta2);

% --------------------------------------------------------
% 6) VARIANCES OF LATENT VARIABLES
%    Double-headed arrows for psi_{11} and psi_{22}
% --------------------------------------------------------
\draw[<->] (eta1) to[out=200, in=230, looseness=4] 
  node[left] {$\psi_{11}$} (eta1);

\draw[<->] (eta2) to[out=-50, in=-20, looseness=4] 
  node[right] {$\psi_{22}$} (eta2);

\foreach \i in {1,...,6}{
  \draw[<->] (e\i)
    to[out=110, in=140, looseness=5]
    node[above] {$\theta_{\i\i}$}
    (e\i);
}

\end{tikzpicture}
```

- SEM parameters include the free entries of $\boldsymbol\nu$, $\boldsymbol\Lambda$, $\boldsymbol\Theta$, $\boldsymbol\alpha$, $\mathbf B$, and $\boldsymbol\Psi$. 

- Dump all in $\vartheta \in\mathbb R^m$, where $m < p(p+1)/2 {\color{Gray} \,+\, p}$.

- Sometimes, not interested in mean structure, so $\boldsymbol\nu$ and $\boldsymbol\alpha$ are dropped.

:::

:::

## ML estimation

- It can be shown that the normal SEM reduces to $\mathbf y\sim \text{N}_p\big(\boldsymbol\mu(\vartheta), \boldsymbol\Sigma(\vartheta)\big)$, where
$$
\begin{align}
\boldsymbol\mu(\vartheta) &= \boldsymbol\nu + \boldsymbol\Lambda (\mathbf I - \mathbf B)^{-1} \boldsymbol\alpha  \\
\boldsymbol\Sigma(\vartheta) &= \boldsymbol\Lambda (\mathbf I - \mathbf B)^{-1} \boldsymbol\Psi (\mathbf I - \mathbf B)^{-\top} \boldsymbol\Lambda^\top + \boldsymbol\Theta
\end{align}
$$

::: {.nudge-down-small}
- Suppose we observe $\mathcal Y= \{\mathbf y_1,\dots,\mathbf y_n\}$. ML estimation maximises (up to a constant) the log-likelihood
  $$
  \ell(\vartheta)
  = -\frac{n}{2}\Bigl[
  \log \bigl|\boldsymbol\Sigma(\vartheta)\bigr|
  + \operatorname{tr} \bigl(\boldsymbol\Sigma(\vartheta)^{-1} \mathbf S\bigr)
  + \bigl(\bar {\mathbf y} - \boldsymbol\mu(\vartheta)\bigr)^{\top}
    \boldsymbol\Sigma(\vartheta)^{-1}
    \bigl(\bar {\mathbf y} - \boldsymbol\mu(\vartheta)\bigr)
  \Bigr]
  $$
  where 
    - $\mathbf S = \frac{1}{n}\sum_{i=1}^n (\mathbf y_i - \bar{\mathbf y})(\mathbf y_i - \bar{\mathbf y})^\top$ is the (biased) sample covariance matrix; and
    - $\bar{\mathbf y} = \frac{1}{n}\sum_{i=1}^n \mathbf y_i$ is the sample mean.

::: {.nudge-down-small}
- Clearly, the MLE aims to minimise the discrepancy between $\mathbf S$ and $\boldsymbol\Sigma(\vartheta)$.
:::

:::

## Properties of MLE

Let $\bar{\vartheta}$ be the true parameter value.
Subject to standard regularity conditions [@cox1979theoretical], as $n\to\infty$,

$$
\sqrt n (\hat\vartheta - \bar\vartheta) \xrightarrow{\;\;\text D\;\;} 
\begin{cases}
\text N_m\left(\mathbf 0, \big[ U(\bar\vartheta)V(\bar\vartheta)^{-1} U(\bar\vartheta) \big]^{-1} \right) &\text{model misspecified} \\
\text{N}_m\big(\mathbf 0, I(\bar\vartheta)^{-1} \big) &\text{otherwise}
\end{cases}
$$
where

- $I(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *Fisher information*;
- $U(\vartheta) = -\mathbb{E}\left[ \nabla\nabla^\top \ell_1(\vartheta) \right]$ is the *sensitivity matrix*; and
- $V(\vartheta) = \mathbb{E}\left[ \nabla\ell_1(\vartheta)\nabla\ell_1(\vartheta)^\top \right]$ is the *variability matrix*.

::: {.nudge-up}
Calculation of SEs are based off estimates of these matrices.
The "sandwich" matrix gives robust SEs [@satorra1994corrections;@savalei2014understanding].
:::


## Latent growth curve model (GCM)

- **Longitudinal data:** repeated measurements on individuals $i$ over time, e.g. $\mathbf y_i = (y_{i1},y_{i2},\dots,y_{i10})$, $i=1,\dots,n$ [@rabe2008multilevel].

- Usually, linear mixed effects models are used, where
$$
\begin{gathered}
y_{it} = \overbrace{(\alpha_1 + \eta_{1i})}^{\text{random int.}} +  \overbrace{(\alpha_2 + \eta_{2i})}^{\text{random slope}} \cdot (t-1) + \epsilon_{it} \quad\quad t=1,\dots,10 \\[0.5em]
\begin{pmatrix}
\eta_{1i} \\
\eta_{2i}
\end{pmatrix}
\sim \text{N}_2\left(\mathbf 0,
\begin{pmatrix}
\psi_{11} & \psi_{12} \\
\cdot & \psi_{22}
\end{pmatrix}\right) \\
\end{gathered}
$$

  ::: {.nudge-down-small}

  - $\alpha_1$ and $\alpha_2$ are fixed effects (intercept and slope);
  - $\eta_1$ and $\eta_2$ are correlated random effects (individual deviations);
  - $\epsilon_t\sim\text{N}(0,\theta)$ are measurement errors.

  :::

::: {.nudge-down-small}
- Restricted ML is a popular method to estimate such models, with good parameter recovery for variance components [@corbeil1976restricted].
:::

## Latent GCM as SEM

```{r, engine = "tikz"}
#| out-width: 85%
\usetikzlibrary{shapes.geometric,arrows,calc,positioning}
\begin{tikzpicture}[%
  >=stealth,              % for arrow tips
  auto,                   % automatic label positioning
  node distance=1.6cm,
  every node/.style={font=\small},
  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},
  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},
  error/.style={circle, draw, minimum size=6mm, inner sep=0mm},
intercept/.style={regular polygon, regular polygon sides=3, draw, inner sep=0pt, minimum size=6mm}
  ]

%-------------------------------------------------------
% 1) LATENT FACTORS (intercept i, slope s)
%-------------------------------------------------------
\node[latent] (i) at (-2,-3) {$\eta_{1}$};
\node[latent] (s) at ( 2,-3) {$\eta_{2}$};
\node[intercept] (int1) at (-2,-4.2) {\footnotesize 1};
\node[intercept] (int2) at (2,-4.2) {\footnotesize 1};

%-------------------------------------------------------
% 2) COVARIANCES AMONG LATENT FACTORS
%    - psi_{11} on i
%    - psi_{22} on s
%    - psi_{21} between i and s
%-------------------------------------------------------
% i <-> s
\draw[<->] (i) to[out=-45, in=225,looseness=0.9] node[below] {$\psi_{12}$} (s);

% Variance of i
\draw[<->] (i) to[out=190, in=220, looseness=4] 
  node[left] {$\psi_{11}$} (i);

% Variance of s
\draw[<->] (s) to[out=-40, in=-10, looseness=4] 
  node[right] {$\psi_{22}$} (s);

\draw[->] (int1) -- node[right,pos=0.3] {$\alpha_1$} (i);
\draw[->] (int2) -- node[right,pos=0.3] {$\alpha_2$} (s);

%-------------------------------------------------------
% 3) OBSERVED VARIABLES (y1..y10) + ERRORS (eps1..eps10)
%    Arrange them in a horizontal row above i and s.
%-------------------------------------------------------
\foreach \j in {1,...,10} {
  % X-position for y_j (shift them so they are roughly centred)
  \pgfmathsetmacro{\x}{(\j*1.3 - 6.5)}

  % Observed variable y_j
  \node[obs] (y\j) at (\x,0) {$y_{\j}$};

  % Error term epsilon_j above y_j
  \node[error] (e\j) at (\x,1.2) {$\epsilon_{\j}$};

  % Arrow from error to observed
  \draw[->] (e\j) -- (y\j);

  % Intercept factor loadings: all = 1
  \draw[->] (i) -- node[pos=0.45,right] {\footnotesize 1} (y\j);

  % Slope factor loadings: 0..9
  \pgfmathtruncatemacro{\loading}{\j - 1}
  \draw[->] (s) -- node[pos=0.7,right] {\footnotesize\loading} (y\j);
}

\foreach \i in {1,...,10}{
  \draw[<->] (e\i)
    to[out=110, in=140, looseness=5]
    node[above] {$\theta$}
    (e\i);
}

\end{tikzpicture}
```



::: {.absolute bottom=2% right=1%}

$$
\begin{gathered}
\boldsymbol\Lambda = \begin{bmatrix}
1 & 0 \\
1 & 1 \\
\vdots & \vdots \\
1 & 9 \\
\end{bmatrix} 
\\
\\
\mathbf y = \mathbf 0 + \boldsymbol\Lambda \boldsymbol\eta + \boldsymbol\epsilon \\
\boldsymbol\epsilon \sim \operatorname{N}_{10}(\mathbf 0, \theta \mathbf I) \\
\boldsymbol\eta \sim \operatorname{N}_2 (\boldsymbol\alpha, \boldsymbol\Psi) 
\end{gathered}
$$
:::





# Bias reduction methods {.transition-slide}

## What is bias?

::: {.callout-tip icon=false}
#### Bias of an estimator
$$
\mathcal B_{\bar\vartheta}(\hat\vartheta) = \mathbb E\left[\hat\vartheta - \bar\vartheta\right] 
$${#eq-bias}
:::


Consider the stochastic Taylor expansion of $s(\hat\vartheta)=\nabla\ell(\vartheta)=0$ around $\bar\vartheta$.
For many common estimators including MLE, the bias function is:
$$
\mathcal B_{\bar\vartheta} = \frac{b_1(\bar\vartheta)}{n} +  \frac{b_2(\bar\vartheta)}{n^2} + \frac{b_3(\bar\vartheta)}{n^3} +O(n^{-4}).
$$

::: {.nudge-up}
Bias arises because the roots of the score equations are **not exactly centred at $\bar\vartheta$**, due to:

::: {.nudge-up-small}
a. The curvature of the score $s(\vartheta)$ creating asymmetry; and
b. The randomness of the score itself.
:::
:::


## Illustration

### Biased MLE estimator for $\sigma^2$ 

Consider $X_1,\dots,X_n \sim \text N(0, \sigma^2)$. The MLE for $\sigma^2$ is $\hat\sigma^2 = \frac{1}{n}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 5)
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 25)
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_bias(n = 1000)
```

:::

## Illustration (cont.)

### Score functions are random too

The score is $s(\sigma^2)=\ell'(\sigma^2) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n X_i^2$.

::: {.panel-tabset}

### $n=10$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
#| fig-align: center
plot_sigma2_score(n = 15) + coord_cartesian(ylim = c(-8, 28))
```

### $n=25$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 50, showbias = FALSE) + coord_cartesian(ylim = c(-8, 300))
```

### $n=1000$

```{r}
#| out-width: 100%
#| fig-height: 2.75
#| fig-width: 7.4
plot_sigma2_score(n = 1000, showbias = FALSE)
```

:::

## If you're interested... 

### ...and love differentiation Ô∏è‚ù§Ô∏èü§ì

For a comprehensive treatment of bias-reduction methods,

- Start here: @cox1968general
- Follow up with: @firth1993bias; @kosmidis2009bias; @kosmidis2014bias


::: {.fragment}
::: {.nudge-up}
By the way, the $O(n^{-1})$ bias term $b_1(\bar\vartheta)/n = -I(\bar\vartheta)^{-1} C(\bar\vartheta)$, where
$$
\begin{gathered}
C_a(\vartheta) = \frac{1}{2} \operatorname{tr} \left[
I(\vartheta)^{-1}\big( G_a(\vartheta) + H_a(\vartheta) \big)
\right]\\ 
G_a(\vartheta)=\mathbb E[s(\vartheta)s(\vartheta)^\top s_a(\vartheta)] 
\quad\quad 
H_a(\vartheta)=-\mathbb E[I(\vartheta)s_a(\vartheta)] \\
a=1,\dots,m
\end{gathered}
$$
where $s(\vartheta) = \nabla\ell(\vartheta)$ is the score function.
:::
:::

## A review

::: {.nudge-up-large}
![](figures/bias.png){fig-align=center width=67%}
:::

::: {.nudge-up-small}
::: {.nudge-up-large}
```{r}
#| html-table-processing: none
library(gt)
library(dplyr)

df <- tribble(
  ~Method, ~Model, ~BG_theta0, ~Type, ~E, ~d, ~theta_hat,
  "Asymptotic bias correction", "full", "analytical", "explicit", "‚úì","‚úì","‚úì",
  "Adjusted score functions", "full", "analytical", "implicit", "‚úì","‚úì","‚úó",
  "Bootstrap", "partial", "simulation", "explicit", "‚úó","‚úó","‚úì",
  "Jackknife", "partial", "simulation", "explicit", "‚úó","‚úó","‚úì",
  "Indirect inference", "full", "simulation", "implicit", "‚úó","‚úó","‚úì",
  "Explicit RBM", "partial", "analytical", "explicit", "‚úó","‚úì","‚úì",
  "Implicit RBM", "partial", "analytical", "implicit", "‚úó","‚úì","‚úó"
)

gt(df, rownames_to_stub = TRUE) |>
  text_transform(
    locations = cells_body(columns = c(E, d, theta_hat)),
    fn = \(x) {
      x |>
        str_replace("‚úì", '<span style="color:#004C59">‚úì</span>') |>
        str_replace("‚úó", '<span style="color:#b10f2e">‚úó</span>')
      # gt::html(out) 
  }) |>
  tab_spanner(md("**Requirements**"), columns = c(E, d, theta_hat)) |>
  cols_label(
    BG_theta0 = md("$\\mathcal{B}(\\bar\\vartheta)$"),
    E = md("$\\mathbb{E}(\\cdot)$"),
    d = md("$\\hspace{2pt} \\partial \\cdot \\hspace{2pt}$"),
    theta_hat = md("$\\hspace{4pt} \\hat\\vartheta \\hspace{4pt}$")
  ) |>
  cols_align(
    align = "center",
    columns = c(E, d, theta_hat)
  ) |>
  tab_style(
    style = "font-weight: bold",
    locations = cells_column_labels()
  ) |>
  tab_options(
    table.font.size = px(25),
    table.width = pct(95)
    # data_row.padding = px(4)
  ) |>
  opt_table_font(
    font = list(
      google_font("Raleway"),  # loads the font for the table
      default_fonts()          # sensible fallbacks
    )
  )
```
:::
:::

::: aside
::: {.footnotesize-text}
1--@efron1975defining, @cordeiro1991bias; 2--@firth1993bias, @kosmidis2009bias; 3--@efron1994introduction, @hall1988bootstrap; 4--@quenouille1956notes, @efron1982jackknife; 5--@gourieroux1993indirect, @mackinnon1998approximate
:::
:::

## Firth's adjusted scores methods

::: {.nudge-up}
Instead of solving [$s(\vartheta)=0$]{.text-org}, solve [$s(\vartheta) + \overbrace{A(\vartheta)}^{\mathcal B(\vartheta) I(\vartheta)}=0$]{.text-tur}.
:::

<!-- ```{r} -->
<!-- #| out-width: 100% -->
<!-- #| fig-align: center -->
<!-- #| fig-height: 4 -->
<!-- #| fig-width: 8 -->
<!-- plot_adjusted_score() -->
<!-- ``` -->

::: {.nudge-up}
![](figures/adjusted_score.gif){fig-align=center width=100%}
:::

## Implicit RBM estimator

Computing $\mathcal B(\vartheta)$ and $I(\vartheta)$ can be difficult. 
Consider

$$
s(\vartheta) + A(\vartheta) = 0 \ \Leftrightarrow \ \underset{\vartheta}{\text{arg max}} \left \{ \ell(\vartheta) + P(\vartheta) \right\}
$${#eq-irbm}

where $P(\vartheta)$ is a penalty term constructed such that $A(\vartheta) = \nabla P(\vartheta)$. 
@kosmidis2024empirical show that
$$
P(\vartheta) = -\frac{1}{2} \operatorname{tr} \Big\{ j(\vartheta)^{-1} e(\vartheta) \Big\}
$$ 

where

::: {.nudge-up-small}
- $j(\vartheta) = \sum_{i=1}^n \nabla\nabla^\top \ell_i(\vartheta)$ is the observed information;
- $e(\vartheta) = \sum_{i=1}^n \nabla\ell_i(\vartheta)\nabla^\top\ell_i(\vartheta)\nabla\ell_i(\vartheta)$ is the outer-product of scores.

::: {.nudge-up-small}
The solution $\tilde\vartheta$ to (-@eq-irbm) is called the [*implicit* RBM estimator (iRBM)]{}.
:::
:::

## Explicit RBM estimator

Intuitively, by thinking in terms of a Newton-style update, an *explicit* estimator is obtained via
$$
\vartheta^*= \hat\vartheta + j(\hat\vartheta)^{-1} A(\hat\vartheta).
$$

This moves $\hat\theta$ in the direction $A(\hat\vartheta)$ away from the bias, with step length governed by the curvature $j(\hat\vartheta)^{-1}$.

::: {.columns}

::: {.column width=50%}
{{< placeholder 600 300 format=svg >}}
:::

::: {.column width=50%}
- Operationally, eRBM is simpler and quicker to compute than iRBM---no re-optimisation needed if $\hat\theta$ is available.

- However, unlike iRBM, no guarantees that bias correction stays inside the parameter space.
:::

:::

<!-- ## Resampling-based methods -->

# Simulation studies {.transition-slide}

## Simulation design

<!-- - Data generating models: -->
<!--    1. Two-factor SEM ($m=13$ estimable params). -->
<!--    2. Latent growth curve model ($m=6$ estimable params). -->
   
- Sample size: $n\in \{15,20,50,100,1000\}$

- Item reliability: Low or High ($\operatorname{Rel} = p^{-1}\sum_{j=1}^p \boldsymbol\Sigma_{jj}^* / \boldsymbol\Sigma_{jj}$)

- Distributional assumption: Normal or Non-normal

```{r}
#| fig-height: 3.6
#| fig-width: 9
#| fig-align: center
#| out-width: 100%
n <- 1000
rho <- 0.3
Sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
dat <- list()
dat$Normal <-
  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(0, 2))[[1]]
## dat$Kurtosis <-  # kurtosis
##   covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(6, 2))[[1]]
dat$`Non-normal` <-  # non-normal
  covsim::rIG(n, Sigma, skewness = rep(-2, 2), excesskurtosis = rep(6, 2))[[1]]

# get max and min for x and y axes
xyminmax <- 
  map(dat, as.data.frame) |>
  bind_rows() |>
  summarise(
    x_min = min(V1),
    x_max = max(V1),
    y_min = min(V2),
    y_max = max(V2)
  ) |> 
  unlist() 

allplots <- imap(dat, \(x, idx) {
  # mycols <- rev(RColorBrewer::brewer.pal(3, "Set1"))
  mycols <- c("#00A6AA",  "#F18F00", "black")
  names(mycols) <- names(dat)
  
  p <-
    as.data.frame(x) |>
    ggplot(aes(x = V1, y = V2)) +
    geom_point(alpha = 0.5, size = 1, col = mycols[idx]) +
    geom_density_2d(col = "black") +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text = element_blank(),
      axis.ticks = element_blank()
    ) +
    scale_x_continuous(limits = c(xyminmax["x_min"], xyminmax["x_max"])) +
    scale_y_continuous(limits = c(xyminmax["y_min"], xyminmax["y_max"])) +
    labs(title = glue::glue("{idx}"), x = NULL, y = NULL)
  ggExtra:::ggMarginal(p, fill = mycols[idx], alpha = 0.5, size = 10, 
                       trim = idx != "Non-normal") 
})
cowplot::plot_grid(plotlist = allplots, nrow = 1)
```


## Results: Two-factor SEM 

::: {.nudge-up-small}

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.5
#| fig-width: 9
p1
```

:::


## Results: Two-factor SEM (cont.)

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.4
#| fig-width: 9
p2
```

## Results: Latent GCM

::: {.nudge-up}

```{r}
#| out-width: 100%
#| fig-align: center
#| fig-height: 5.4
#| fig-width: 9
p3
```

:::

# Conclusion {.transition-slide}

## Summary & future work

RBM applied to small sample estimation of SEM show key advantages:

- Improved estimator performances (mean & median bias, RMSE, coverage).

- Computationally efficient (c.f. resampling methods).

- Robust to model misspecification.

Future work include

1. Computational improvements for iRBM.

2. Plugin penalties to limit exploration of ill-conditioned regions.

3. Extension to other SEM families, e.g.
   - Mediation models, latent interactions, etc.
   - Alternative to ML estimation e.g. WLS, DWLS, etc.

## Software 

```{r}
#| include: false
library(brlavaan)
dat <- brlavaan::gen_data_twofac(n = 50)
colnames(dat) <- paste0("y", 1:6)
options(width = 180)
```


```{r}
#| echo: true
#| cache: true
# pak::pak("haziqj/brlavaan")
library(brlavaan)

mod <- "
  eta1 =~ y1 + y2 + y3
  eta2 =~ y4 + y5 + y6
"
fit <- brsem(model = mod, data = dat, estimator.args = list(rbm = "implicit"))
summary(fit)
```

# ÿ¥ŸÉÿ±ÿßŸã ÿ¨ÿ≤ŸäŸÑÿßŸã {.thanks-slide  background-image="_extensions/haziqj/kaust/KAUST-Thank-you.jpg" style="padding-top:0.5em;padding-bottom:0em"}

[`https://haziqj.ml/sembias-gradsem`](https://haziqj.ml/sembias-gradsem)

## References